{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac328150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for dataset: lotte/lifestyle/dev/forum\n",
      "Execution Time (seconds): 301.16\n",
      "Average Precision: 0.4971\n",
      "Average Recall: 0.4971\n",
      "Average MAP Score: 0.5649\n",
      "Average MRR: 0.5903\n"
     ]
    }
   ],
   "source": [
    "# خلية 1: استيراد المكتبات اللازمة\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import json\n",
    "import asyncio\n",
    "import httpx\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 2: TextProcessor كما سبق (مع تعديل دالة number_to_words)\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def cleaned_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalization_example(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def stemming_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    def lemmatization_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def number_to_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        converted_words = []\n",
    "        for word in words:\n",
    "            # تحقق من أن الكلمة أرقام عادية فقط\n",
    "            if word.isdecimal() and word.isascii():\n",
    "                try:\n",
    "                    num = int(word)\n",
    "                    if num <= 999999999999999:\n",
    "                        converted_word = self.inflect_engine.number_to_words(word)\n",
    "                        converted_words.append(converted_word)\n",
    "                    else:\n",
    "                        converted_words.append(\"[Number Out of Range]\")\n",
    "                except (ValueError, inflect.NumOutOfRangeError):\n",
    "                    converted_words.append(\"[Number Out of Range]\")\n",
    "            else:\n",
    "                converted_words.append(word)\n",
    "        return ' '.join(converted_words)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        negated_text = []\n",
    "        negate = False\n",
    "        for word in words:\n",
    "            if word.lower() in ['not', \"n't\"]:\n",
    "                negate = True\n",
    "            elif negate:\n",
    "                negated_text.append(f\"NOT_{word}\")\n",
    "                negate = False\n",
    "            else:\n",
    "                negated_text.append(word)\n",
    "        return ' '.join(negated_text)\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if text is None:\n",
    "            return text\n",
    "        text = self.cleaned_text(text)\n",
    "        text = self.normalization_example(text)\n",
    "        text = self.stemming_example(text)\n",
    "        text = self.lemmatization_example(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.number_to_words(text)\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.normalize_unicode(text)\n",
    "        text = self.handle_negations(text)\n",
    "        text = self.remove_urls(text)\n",
    "        return text\n",
    "\n",
    "processor = TextProcessor()\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 3: جلب البيانات من MongoDB\n",
    "\n",
    "def get_data_from_mongo(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection_name = dataset_path.replace(\"/\", \"_\")\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    pids = []\n",
    "    texts = []\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc and isinstance(doc[\"text\"], str):\n",
    "            pids.append(str(doc[\"doc_id\"]))\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "    df = pd.DataFrame({\"pid\": pids, \"text\": texts})\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    return df\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 4: بناء TF-IDF في الذاكرة\n",
    "\n",
    "def build_tfidf_in_memory(df):\n",
    "    vectorizer = TfidfVectorizer(preprocessor=processor.preprocess, max_df=0.5, min_df=1)\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 5: البحث في TF-IDF\n",
    "\n",
    "def search_in_tfidf(query, vectorizer, tfidf_matrix, df, top_n=10):\n",
    "    processed_query = processor.preprocess(query)\n",
    "    query_vector = vectorizer.transform([processed_query])\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix, query_vector).flatten()\n",
    "    top_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    top_docs = df.iloc[top_indices]\n",
    "    results = {\n",
    "        \"top_documents\": top_docs.to_dict(orient=\"records\"),\n",
    "        \"cosine_similarities\": cosine_similarities[top_indices].tolist(),\n",
    "        \"top_documents_indices\": top_indices.tolist()\n",
    "    }\n",
    "    return results\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 6: دوال التقييم\n",
    "\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_map_scores = []\n",
    "all_mrrs = []\n",
    "\n",
    "def calculate_precision_recall(relevantOrNot, retrievedDocument, threshold=0.5):\n",
    "    binaryResult = (retrievedDocument >= threshold).astype(int)\n",
    "    precision = precision_score(relevantOrNot, binaryResult, average='micro')\n",
    "    recall = recall_score(relevantOrNot, binaryResult, average='micro')\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_map_score(relevantOrNot, retrievedDocument):\n",
    "    return average_precision_score(relevantOrNot, retrievedDocument, average='micro')\n",
    "\n",
    "def calculate_mrr(y_true):\n",
    "    rank_position = np.where(y_true == 1)[0]\n",
    "    if len(rank_position) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (rank_position[0] + 1)\n",
    "\n",
    "def load_queries(queries_paths):\n",
    "    queries = []\n",
    "    for file_path in queries_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    query = json.loads(line.strip())\n",
    "                    if 'query' in query:\n",
    "                        queries.append(query)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "    return queries\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 7: تقييم بحث TF-IDF (يرجى تعديل search_function حسب حاجتك)\n",
    "\n",
    "def evaluate_search(dataset_path, search_function):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = get_data_from_mongo(dataset_path)\n",
    "    \n",
    "    queries_paths = ''\n",
    "    if dataset_path == 'lotte/lifestyle/dev/forum':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\qas.search.jsonl'\n",
    "    elif dataset_path == 'antique/train':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\antique\\test\\Answers.jsonl'\n",
    "    else:\n",
    "        print(\"Warning: No queries path configured for this dataset.\")\n",
    "        return\n",
    "    \n",
    "    queries = load_queries([queries_paths])\n",
    "\n",
    "    for query in queries:\n",
    "        if 'query' not in query:\n",
    "            continue\n",
    "        \n",
    "        # استدعاء دالة البحث مع الباراميترات المناسبة\n",
    "        response_json = search_function(query['query'], top_n=10)\n",
    "        \n",
    "        top_documents = response_json[\"top_documents\"]\n",
    "        cosine_similarities = np.array(response_json[\"cosine_similarities\"])\n",
    "        top_documents_indices = response_json[\"top_documents_indices\"]\n",
    "\n",
    "        relevance = np.zeros(len(df))\n",
    "\n",
    "        for pid in query.get('answer_pids', []):\n",
    "            pid_str = str(pid)\n",
    "            indices = np.where(df['pid'] == pid_str)[0]\n",
    "            relevance[indices] = 1\n",
    "\n",
    "        retrievedDocument = cosine_similarities\n",
    "        relevantOrNot = relevance[top_documents_indices]\n",
    "\n",
    "        if relevantOrNot.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        precision, recall = calculate_precision_recall(relevantOrNot, retrievedDocument)\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "        map_score = calculate_map_score(relevantOrNot, retrievedDocument)\n",
    "        all_map_scores.append(map_score)\n",
    "\n",
    "        mrr = calculate_mrr(relevantOrNot)\n",
    "        all_mrrs.append(mrr)\n",
    "\n",
    "    if len(all_precisions) == 0:\n",
    "        print(\"⚠️ No valid queries evaluated. Check PIDs matching and dataset content.\")\n",
    "        return\n",
    "\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_map_score = np.mean(all_map_scores)\n",
    "    avg_mrr = np.mean(all_mrrs)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Evaluation results for dataset: {dataset_path}\")\n",
    "    print(f\"Execution Time (seconds): {elapsed_time:.2f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average MAP Score: {avg_map_score:.4f}\")\n",
    "    print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 8: مثال كامل للاستخدام (تعديل حسب حاجتك)\n",
    "\n",
    "dataset_path = \"lotte/lifestyle/dev/forum\"  # غير هذا حسب بياناتك\n",
    "\n",
    "df = get_data_from_mongo(dataset_path)\n",
    "vectorizer, tfidf_matrix = build_tfidf_in_memory(df)\n",
    "\n",
    "def search_function(query, top_n=10):\n",
    "    return search_in_tfidf(query, vectorizer, tfidf_matrix, df, top_n)\n",
    "\n",
    "# تشغيل التقييم (في حال أردت)\n",
    "\n",
    "evaluate_search(dataset_path, search_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d97d58d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for dataset: lotte/lifestyle/dev/forum\n",
      "Execution Time (seconds): 7989.27\n",
      "Average Precision: 0.4691\n",
      "Average Recall: 0.4691\n",
      "Average MAP Score: 0.7245\n",
      "Average MRR: 0.7826\n"
     ]
    }
   ],
   "source": [
    "# خلية 1: استيراد المكتبات اللازمة\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 2: TextProcessor كما سبق (مع تعديل دالة number_to_words)\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def cleaned_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalization_example(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def stemming_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    def lemmatization_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def number_to_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        converted_words = []\n",
    "        for word in words:\n",
    "            # تحقق من أن الكلمة أرقام عادية فقط\n",
    "            if word.isdecimal() and word.isascii():\n",
    "                try:\n",
    "                    num = int(word)\n",
    "                    if num <= 999999999999999:\n",
    "                        converted_word = self.inflect_engine.number_to_words(word)\n",
    "                        converted_words.append(converted_word)\n",
    "                    else:\n",
    "                        converted_words.append(\"[Number Out of Range]\")\n",
    "                except (ValueError, inflect.NumOutOfRangeError):\n",
    "                    converted_words.append(\"[Number Out of Range]\")\n",
    "            else:\n",
    "                converted_words.append(word)\n",
    "        return ' '.join(converted_words)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        negated_text = []\n",
    "        negate = False\n",
    "        for word in words:\n",
    "            if word.lower() in ['not', \"n't\"]:\n",
    "                negate = True\n",
    "            elif negate:\n",
    "                negated_text.append(f\"NOT_{word}\")\n",
    "                negate = False\n",
    "            else:\n",
    "                negated_text.append(word)\n",
    "        return ' '.join(negated_text)\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if text is None:\n",
    "            return text\n",
    "        text = self.cleaned_text(text)\n",
    "        text = self.normalization_example(text)\n",
    "        text = self.stemming_example(text)\n",
    "        text = self.lemmatization_example(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.number_to_words(text)\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.normalize_unicode(text)\n",
    "        text = self.handle_negations(text)\n",
    "        text = self.remove_urls(text)\n",
    "        return text\n",
    "\n",
    "processor = TextProcessor()\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 3: جلب البيانات من MongoDB\n",
    "\n",
    "def get_data_from_mongo(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection_name = dataset_path.replace(\"/\", \"_\")\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    pids = []\n",
    "    texts = []\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc and isinstance(doc[\"text\"], str):\n",
    "            pids.append(str(doc[\"doc_id\"]))\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "    df = pd.DataFrame({\"pid\": pids, \"text\": texts})\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    return df\n",
    "\n",
    "#---\n",
    "\n",
    "# تحميل الموديلات خارج الدالة حتى تكون مرة وحدة\n",
    "retrieval_model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "processor = TextProcessor()\n",
    "\n",
    "# تحميل FAISS index والـ doc_ids\n",
    "def get_faiss_index_and_doc_ids(dataset_path: str):\n",
    "    db_dir = os.path.join(r\"C:\\Users\\USER\\Desktop\\IR_Final_Project\\db\", dataset_path.replace(\"/\", \"__\"))\n",
    "    index = faiss.read_index(os.path.join(db_dir, \"bert_faiss.index\"))\n",
    "    doc_ids = joblib.load(os.path.join(db_dir, \"bert_doc_ids.joblib\"))\n",
    "    return index, doc_ids\n",
    "\n",
    "# تحميل كل الوثائق كماب: doc_id → text\n",
    "def load_documents_map(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection = db[dataset_path.replace(\"/\", \"_\")]\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    doc_map = {}\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc:\n",
    "            doc_map[str(doc[\"doc_id\"])] = doc[\"text\"]\n",
    "    return doc_map\n",
    "\n",
    "# دالة البحث باستخدام BERT + FAISS\n",
    "def search_in_bert(query, dataset_path, top_k=50, rerank_k=10):\n",
    "    index, doc_ids = get_faiss_index_and_doc_ids(dataset_path)\n",
    "    doc_map = load_documents_map(dataset_path)\n",
    "    doc_id_to_index = {str(doc_id): i for i, doc_id in enumerate(doc_ids)}\n",
    "\n",
    "    query_processed = processor.preprocess(query)\n",
    "    query_vec = retrieval_model.encode(query_processed, normalize_embeddings=True).astype(np.float32).reshape(1, -1)\n",
    "    faiss.normalize_L2(query_vec)\n",
    "\n",
    "    scores, indices = index.search(query_vec, top_k)\n",
    "    top_doc_ids = [doc_ids[i] for i in indices[0]]\n",
    "    top_docs = [(str(doc_id), doc_map.get(str(doc_id), \"\")) for doc_id in top_doc_ids]\n",
    "\n",
    "    filtered_docs = [(doc_id, text) for doc_id, text in top_docs if text.strip()]\n",
    "    pairs = [(query, text) for _, text in filtered_docs]\n",
    "    rerank_scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    ranked = sorted(zip(filtered_docs, rerank_scores), key=lambda x: x[1], reverse=True)\n",
    "    reranked = ranked[:rerank_k]\n",
    "\n",
    "    top_documents = []\n",
    "    cosine_similarities = []\n",
    "    top_documents_indices = []\n",
    "\n",
    "    for (doc_id, text), score in reranked:\n",
    "        top_documents.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"score\": float(score),\n",
    "            \"text\": text\n",
    "        })\n",
    "        cosine_similarities.append(float(score))\n",
    "        top_documents_indices.append(doc_id_to_index.get(doc_id, -1))\n",
    "\n",
    "    return {\n",
    "        \"top_documents\": top_documents,\n",
    "        \"cosine_similarities\": cosine_similarities,\n",
    "        \"top_documents_indices\": top_documents_indices\n",
    "    }\n",
    "\n",
    "\n",
    "# خلية 6: دوال التقييم\n",
    "\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_map_scores = []\n",
    "all_mrrs = []\n",
    "\n",
    "def calculate_precision_recall(relevantOrNot, retrievedDocument, threshold=0.5):\n",
    "    binaryResult = (retrievedDocument >= threshold).astype(int)\n",
    "    precision = precision_score(relevantOrNot, binaryResult, average='micro')\n",
    "    recall = recall_score(relevantOrNot, binaryResult, average='micro')\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_map_score(relevantOrNot, retrievedDocument):\n",
    "    return average_precision_score(relevantOrNot, retrievedDocument, average='micro')\n",
    "\n",
    "def calculate_mrr(y_true):\n",
    "    rank_position = np.where(y_true == 1)[0]\n",
    "    if len(rank_position) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (rank_position[0] + 1)\n",
    "\n",
    "def load_queries(queries_paths):\n",
    "    queries = []\n",
    "    for file_path in queries_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    query = json.loads(line.strip())\n",
    "                    if 'query' in query:\n",
    "                        queries.append(query)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "    return queries\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 7: تقييم بحث TF-IDF (يرجى تعديل search_function حسب حاجتك)\n",
    "\n",
    "def evaluate_search(dataset_path, search_function):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = get_data_from_mongo(dataset_path)\n",
    "    \n",
    "    queries_paths = ''\n",
    "    if dataset_path == 'lotte/lifestyle/dev/forum':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\qas.search.jsonl'\n",
    "    elif dataset_path == 'antique/train':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\antique\\test\\Answers.jsonl'\n",
    "    else:\n",
    "        print(\"Warning: No queries path configured for this dataset.\")\n",
    "        return\n",
    "    \n",
    "    queries = load_queries([queries_paths])\n",
    "\n",
    "    for query in queries:\n",
    "        if 'query' not in query:\n",
    "            continue\n",
    "        \n",
    "        # استدعاء دالة البحث مع الباراميترات المناسبة\n",
    "        response_json = search_function(query['query'], top_n=10)\n",
    "        \n",
    "        top_documents = response_json[\"top_documents\"]\n",
    "        cosine_similarities = np.array(response_json[\"cosine_similarities\"])\n",
    "        top_documents_indices = response_json[\"top_documents_indices\"]\n",
    "\n",
    "        relevance = np.zeros(len(df))\n",
    "\n",
    "        for pid in query.get('answer_pids', []):\n",
    "            pid_str = str(pid)\n",
    "            indices = np.where(df['pid'] == pid_str)[0]\n",
    "            relevance[indices] = 1\n",
    "\n",
    "        retrievedDocument = cosine_similarities\n",
    "        relevantOrNot = relevance[top_documents_indices]\n",
    "\n",
    "        if relevantOrNot.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        precision, recall = calculate_precision_recall(relevantOrNot, retrievedDocument)\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "        map_score = calculate_map_score(relevantOrNot, retrievedDocument)\n",
    "        all_map_scores.append(map_score)\n",
    "\n",
    "        mrr = calculate_mrr(relevantOrNot)\n",
    "        all_mrrs.append(mrr)\n",
    "\n",
    "    if len(all_precisions) == 0:\n",
    "        print(\"⚠️ No valid queries evaluated. Check PIDs matching and dataset content.\")\n",
    "        return\n",
    "\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_map_score = np.mean(all_map_scores)\n",
    "    avg_mrr = np.mean(all_mrrs)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Evaluation results for dataset: {dataset_path}\")\n",
    "    print(f\"Execution Time (seconds): {elapsed_time:.2f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average MAP Score: {avg_map_score:.4f}\")\n",
    "    print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 8: مثال كامل للاستخدام (تعديل حسب حاجتك)\n",
    "\n",
    "dataset_path = \"lotte/lifestyle/dev/forum\"  \n",
    "\n",
    "def search_function(query, top_n=10):\n",
    "    return search_in_bert(query, dataset_path, top_k=50, rerank_k=top_n)\n",
    "\n",
    "# تشغيل التقييم\n",
    "evaluate_search(dataset_path, search_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70cd64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# خلية 1: استيراد المكتبات اللازمة\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 2: TextProcessor كما سبق (مع تعديل دالة number_to_words)\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def cleaned_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalization_example(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def stemming_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    def lemmatization_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def number_to_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        converted_words = []\n",
    "        for word in words:\n",
    "            # تحقق من أن الكلمة أرقام عادية فقط\n",
    "            if word.isdecimal() and word.isascii():\n",
    "                try:\n",
    "                    num = int(word)\n",
    "                    if num <= 999999999999999:\n",
    "                        converted_word = self.inflect_engine.number_to_words(word)\n",
    "                        converted_words.append(converted_word)\n",
    "                    else:\n",
    "                        converted_words.append(\"[Number Out of Range]\")\n",
    "                except (ValueError, inflect.NumOutOfRangeError):\n",
    "                    converted_words.append(\"[Number Out of Range]\")\n",
    "            else:\n",
    "                converted_words.append(word)\n",
    "        return ' '.join(converted_words)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        negated_text = []\n",
    "        negate = False\n",
    "        for word in words:\n",
    "            if word.lower() in ['not', \"n't\"]:\n",
    "                negate = True\n",
    "            elif negate:\n",
    "                negated_text.append(f\"NOT_{word}\")\n",
    "                negate = False\n",
    "            else:\n",
    "                negated_text.append(word)\n",
    "        return ' '.join(negated_text)\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if text is None:\n",
    "            return text\n",
    "        text = self.cleaned_text(text)\n",
    "        text = self.normalization_example(text)\n",
    "        text = self.stemming_example(text)\n",
    "        text = self.lemmatization_example(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.number_to_words(text)\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.normalize_unicode(text)\n",
    "        text = self.handle_negations(text)\n",
    "        text = self.remove_urls(text)\n",
    "        return text\n",
    "\n",
    "processor = TextProcessor()\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 3: جلب البيانات من MongoDB\n",
    "\n",
    "def get_data_from_mongo(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection_name = dataset_path.replace(\"/\", \"_\")\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    pids = []\n",
    "    texts = []\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc and isinstance(doc[\"text\"], str):\n",
    "            pids.append(str(doc[\"doc_id\"]))\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "    df = pd.DataFrame({\"pid\": pids, \"text\": texts})\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    return df\n",
    "\n",
    "from functools import lru_cache\n",
    "from sklearn.decomposition import TruncatedSVD  # إذا مش موجود ضمن ملف joblib، لكن ما رح نعمل import جديد هنا حسب طلبك\n",
    "\n",
    "# كاش لتحميل المودل\n",
    "@lru_cache(maxsize=1)\n",
    "def load_bert_model():\n",
    "    return SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "# كاش عام للملفات الثقيلة\n",
    "_loaded_cache = {}\n",
    "\n",
    "def load_cached(path, loader=joblib.load):\n",
    "    if path not in _loaded_cache:\n",
    "        _loaded_cache[path] = loader(path)\n",
    "    return _loaded_cache[path]\n",
    "\n",
    "def search_in_hybrid(query, dataset_path, top_n=10, tfidf_weight=0.4, bert_weight=0.6):\n",
    "    safe_name = dataset_path.replace(\"/\", \"__\")\n",
    "    db_dir = os.path.join(\"db\", safe_name)\n",
    "\n",
    "    tfidf_vectorizer = load_cached(os.path.join(db_dir, \"vectorizer.joblib\"))\n",
    "    tfidf_matrix = load_cached(os.path.join(db_dir, \"tfidf_matrix.joblib\"))\n",
    "    docs_df = load_cached(os.path.join(db_dir, \"docs.joblib\"))\n",
    "    bert_embeddings = load_cached(os.path.join(db_dir, \"bert_embeddings.joblib\"))\n",
    "    bert_doc_ids = load_cached(os.path.join(db_dir, \"bert_doc_ids.joblib\"))\n",
    "    svd = load_cached(os.path.join(db_dir, \"svd_model.joblib\"))\n",
    "\n",
    "    query_processed = processor.preprocess(query)\n",
    "\n",
    "    tfidf_q = tfidf_vectorizer.transform([query_processed])\n",
    "    tfidf_q_reduced = svd.transform(tfidf_q)\n",
    "\n",
    "    model = load_bert_model()\n",
    "    bert_q = model.encode([query_processed], normalize_embeddings=True)\n",
    "\n",
    "    min_dim = min(tfidf_q_reduced.shape[1], bert_q.shape[1])\n",
    "    tfidf_q_reduced_cut = tfidf_q_reduced[:, :min_dim]\n",
    "    bert_q_cut = bert_q[:, :min_dim]\n",
    "\n",
    "    hybrid_query = tfidf_weight * tfidf_q_reduced_cut + bert_weight * bert_q_cut\n",
    "    hybrid_query = np.ascontiguousarray(hybrid_query.astype(np.float32))\n",
    "    faiss.normalize_L2(hybrid_query)\n",
    "\n",
    "    index = faiss.read_index(os.path.join(db_dir, \"hybrid_faiss.index\"))\n",
    "    D, I = index.search(hybrid_query, top_n)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        results.append({\n",
    "            \"doc_id\": docs_df.iloc[idx][\"pid\"],\n",
    "            \"score\": float(score),\n",
    "            \"text\": docs_df.iloc[idx][\"text\"]\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"top_documents\": results,\n",
    "        \"cosine_similarities\": D[0].tolist(),\n",
    "        \"top_documents_indices\": I[0].tolist()\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# خلية 6: دوال التقييم\n",
    "\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_map_scores = []\n",
    "all_mrrs = []\n",
    "\n",
    "def calculate_precision_recall(relevantOrNot, retrievedDocument, threshold=0.5):\n",
    "    binaryResult = (retrievedDocument >= threshold).astype(int)\n",
    "    precision = precision_score(relevantOrNot, binaryResult, average='micro')\n",
    "    recall = recall_score(relevantOrNot, binaryResult, average='micro')\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_map_score(relevantOrNot, retrievedDocument):\n",
    "    return average_precision_score(relevantOrNot, retrievedDocument, average='micro')\n",
    "\n",
    "def calculate_mrr(y_true):\n",
    "    rank_position = np.where(y_true == 1)[0]\n",
    "    if len(rank_position) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (rank_position[0] + 1)\n",
    "\n",
    "def load_queries(queries_paths):\n",
    "    queries = []\n",
    "    for file_path in queries_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    query = json.loads(line.strip())\n",
    "                    if 'query' in query:\n",
    "                        queries.append(query)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "    return queries\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 7: تقييم بحث TF-IDF (يرجى تعديل search_function حسب حاجتك)\n",
    "\n",
    "def evaluate_search(dataset_path, search_function):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = get_data_from_mongo(dataset_path)\n",
    "    \n",
    "    queries_paths = ''\n",
    "    if dataset_path == 'lotte/lifestyle/dev/forum':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\qas.search.jsonl'\n",
    "    elif dataset_path == 'antique/train':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\antique\\test\\Answers.jsonl'\n",
    "    else:\n",
    "        print(\"Warning: No queries path configured for this dataset.\")\n",
    "        return\n",
    "    \n",
    "    queries = load_queries([queries_paths])\n",
    "\n",
    "    for query in queries:\n",
    "        if 'query' not in query:\n",
    "            continue\n",
    "        \n",
    "        # استدعاء دالة البحث مع الباراميترات المناسبة\n",
    "        response_json = search_function(query['query'], top_n=10)\n",
    "        \n",
    "        top_documents = response_json[\"top_documents\"]\n",
    "        cosine_similarities = np.array(response_json[\"cosine_similarities\"])\n",
    "        top_documents_indices = response_json[\"top_documents_indices\"]\n",
    "\n",
    "        relevance = np.zeros(len(df))\n",
    "\n",
    "        for pid in query.get('answer_pids', []):\n",
    "            pid_str = str(pid)\n",
    "            indices = np.where(df['pid'] == pid_str)[0]\n",
    "            relevance[indices] = 1\n",
    "\n",
    "        retrievedDocument = cosine_similarities\n",
    "        relevantOrNot = relevance[top_documents_indices]\n",
    "\n",
    "        if relevantOrNot.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        precision, recall = calculate_precision_recall(relevantOrNot, retrievedDocument)\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "        map_score = calculate_map_score(relevantOrNot, retrievedDocument)\n",
    "        all_map_scores.append(map_score)\n",
    "\n",
    "        mrr = calculate_mrr(relevantOrNot)\n",
    "        all_mrrs.append(mrr)\n",
    "\n",
    "    if len(all_precisions) == 0:\n",
    "        print(\"⚠️ No valid queries evaluated. Check PIDs matching and dataset content.\")\n",
    "        return\n",
    "\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_map_score = np.mean(all_map_scores)\n",
    "    avg_mrr = np.mean(all_mrrs)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Evaluation results for dataset: {dataset_path}\")\n",
    "    print(f\"Execution Time (seconds): {elapsed_time:.2f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average MAP Score: {avg_map_score:.4f}\")\n",
    "    print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 8: مثال كامل للاستخدام (تعديل حسب حاجتك)\n",
    "\n",
    "dataset_path = \"lotte/lifestyle/dev/forum\"  \n",
    "\n",
    "def search_function(query, top_n=10):\n",
    "    return search_in_hybrid(query, dataset_path, top_n=top_n, tfidf_weight=0.4, bert_weight=0.6)\n",
    "\n",
    "# تشغيل التقييم\n",
    "evaluate_search(dataset_path, search_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# خلية 1: استيراد المكتبات اللازمة\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 2: TextProcessor كما سبق (مع تعديل دالة number_to_words)\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def cleaned_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalization_example(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def stemming_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    def lemmatization_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def number_to_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        converted_words = []\n",
    "        for word in words:\n",
    "            # تحقق من أن الكلمة أرقام عادية فقط\n",
    "            if word.isdecimal() and word.isascii():\n",
    "                try:\n",
    "                    num = int(word)\n",
    "                    if num <= 999999999999999:\n",
    "                        converted_word = self.inflect_engine.number_to_words(word)\n",
    "                        converted_words.append(converted_word)\n",
    "                    else:\n",
    "                        converted_words.append(\"[Number Out of Range]\")\n",
    "                except (ValueError, inflect.NumOutOfRangeError):\n",
    "                    converted_words.append(\"[Number Out of Range]\")\n",
    "            else:\n",
    "                converted_words.append(word)\n",
    "        return ' '.join(converted_words)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        negated_text = []\n",
    "        negate = False\n",
    "        for word in words:\n",
    "            if word.lower() in ['not', \"n't\"]:\n",
    "                negate = True\n",
    "            elif negate:\n",
    "                negated_text.append(f\"NOT_{word}\")\n",
    "                negate = False\n",
    "            else:\n",
    "                negated_text.append(word)\n",
    "        return ' '.join(negated_text)\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if text is None:\n",
    "            return text\n",
    "        text = self.cleaned_text(text)\n",
    "        text = self.normalization_example(text)\n",
    "        text = self.stemming_example(text)\n",
    "        text = self.lemmatization_example(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.number_to_words(text)\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.normalize_unicode(text)\n",
    "        text = self.handle_negations(text)\n",
    "        text = self.remove_urls(text)\n",
    "        return text\n",
    "\n",
    "processor = TextProcessor()\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 3: جلب البيانات من MongoDB\n",
    "\n",
    "def get_data_from_mongo(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection_name = dataset_path.replace(\"/\", \"_\")\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    pids = []\n",
    "    texts = []\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc and isinstance(doc[\"text\"], str):\n",
    "            pids.append(str(doc[\"doc_id\"]))\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "    df = pd.DataFrame({\"pid\": pids, \"text\": texts})\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    return df\n",
    "\n",
    "from functools import lru_cache\n",
    "from sklearn.decomposition import TruncatedSVD  # إذا مش موجود ضمن ملف joblib، لكن ما رح نعمل import جديد هنا حسب طلبك\n",
    "\n",
    "# كاش لتحميل المودل\n",
    "@lru_cache(maxsize=1)\n",
    "def load_bert_model():\n",
    "    return SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "# كاش عام للملفات الثقيلة\n",
    "_loaded_cache = {}\n",
    "\n",
    "def load_cached(path, loader=joblib.load):\n",
    "    if path not in _loaded_cache:\n",
    "        _loaded_cache[path] = loader(path)\n",
    "    return _loaded_cache[path]\n",
    "\n",
    "def search_in_bm25(query, dataset_path, top_k=10, initial_k=30):\n",
    "    logger.info(f\"[BM25 Eval] BM25 + Cross-Encoder Re-ranking on dataset: {dataset_path}\")\n",
    "\n",
    "    bm25_model, doc_ids, tokenized_texts = get_bm25_components(dataset_path)\n",
    "\n",
    "    query_tokens = bm25_processed_text(query)\n",
    "    if not query_tokens:\n",
    "        logger.warning(\"Query processing resulted in no tokens.\")\n",
    "        return {\"top_documents\": [], \"cosine_similarities\": [], \"top_documents_indices\": []}\n",
    "\n",
    "    expanded_query = expand_query(query_tokens)\n",
    "    weighted_index = get_weighted_index(dataset_path)\n",
    "\n",
    "    candidate_doc_ids = set()\n",
    "    for term in expanded_query:\n",
    "        if term in weighted_index:\n",
    "            for entry in weighted_index[term]:\n",
    "                if isinstance(entry, dict):\n",
    "                    candidate_doc_ids.add(entry[\"doc_id\"])\n",
    "                else:\n",
    "                    candidate_doc_ids.add(entry)\n",
    "\n",
    "    if not candidate_doc_ids:\n",
    "        logger.warning(\"No matching documents found in weighted inverted index.\")\n",
    "        return {\"top_documents\": [], \"cosine_similarities\": [], \"top_documents_indices\": []}\n",
    "\n",
    "    doc_id_to_index = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
    "    candidate_indices = [doc_id_to_index[doc_id] for doc_id in candidate_doc_ids if doc_id in doc_id_to_index]\n",
    "\n",
    "    if not candidate_indices:\n",
    "        logger.warning(\"No candidate indices matched in BM25 model.\")\n",
    "        return {\"top_documents\": [], \"cosine_similarities\": [], \"top_documents_indices\": []}\n",
    "\n",
    "    scores = bm25_model.get_scores(expanded_query)\n",
    "    candidate_scores = [(i, scores[i]) for i in candidate_indices]\n",
    "    candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_candidates = candidate_scores[:initial_k]\n",
    "    top_indices = [i for i, _ in top_candidates]\n",
    "    top_doc_ids = [doc_ids[i] for i in top_indices]\n",
    "    doc_texts = load_documents_by_ids(dataset_path, top_doc_ids)\n",
    "\n",
    "    cross_encoder = get_cross_encoder()\n",
    "    cross_inputs = [(query, doc_texts[doc_ids[i]]) for i in top_indices]\n",
    "    rerank_scores = cross_encoder.predict(cross_inputs, batch_size=16)\n",
    "\n",
    "    reranked = sorted(zip(top_doc_ids, rerank_scores, top_indices), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    top_documents = []\n",
    "    cosine_similarities = []\n",
    "    top_documents_indices = []\n",
    "\n",
    "    for doc_id, score, idx in reranked:\n",
    "        top_documents.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"score\": float(score),\n",
    "            \"text\": doc_texts[doc_id]\n",
    "        })\n",
    "        cosine_similarities.append(float(score))\n",
    "        top_documents_indices.append(idx)\n",
    "\n",
    "    return {\n",
    "        \"top_documents\": top_documents,\n",
    "        \"cosine_similarities\": cosine_similarities,\n",
    "        \"top_documents_indices\": top_documents_indices\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# خلية 6: دوال التقييم\n",
    "\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_map_scores = []\n",
    "all_mrrs = []\n",
    "\n",
    "def calculate_precision_recall(relevantOrNot, retrievedDocument, threshold=0.5):\n",
    "    binaryResult = (retrievedDocument >= threshold).astype(int)\n",
    "    precision = precision_score(relevantOrNot, binaryResult, average='micro')\n",
    "    recall = recall_score(relevantOrNot, binaryResult, average='micro')\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_map_score(relevantOrNot, retrievedDocument):\n",
    "    return average_precision_score(relevantOrNot, retrievedDocument, average='micro')\n",
    "\n",
    "def calculate_mrr(y_true):\n",
    "    rank_position = np.where(y_true == 1)[0]\n",
    "    if len(rank_position) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (rank_position[0] + 1)\n",
    "\n",
    "def load_queries(queries_paths):\n",
    "    queries = []\n",
    "    for file_path in queries_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    query = json.loads(line.strip())\n",
    "                    if 'query' in query:\n",
    "                        queries.append(query)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "    return queries\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 7: تقييم بحث TF-IDF (يرجى تعديل search_function حسب حاجتك)\n",
    "\n",
    "def evaluate_search(dataset_path, search_function):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = get_data_from_mongo(dataset_path)\n",
    "    \n",
    "    queries_paths = ''\n",
    "    if dataset_path == 'lotte/lifestyle/dev/forum':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\qas.search.jsonl'\n",
    "    elif dataset_path == 'antique/train':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\antique\\test\\Answers.jsonl'\n",
    "    else:\n",
    "        print(\"Warning: No queries path configured for this dataset.\")\n",
    "        return\n",
    "    \n",
    "    queries = load_queries([queries_paths])\n",
    "\n",
    "    for query in queries:\n",
    "        if 'query' not in query:\n",
    "            continue\n",
    "        \n",
    "        # استدعاء دالة البحث مع الباراميترات المناسبة\n",
    "        response_json = search_function(query['query'], top_n=10)\n",
    "        \n",
    "        top_documents = response_json[\"top_documents\"]\n",
    "        cosine_similarities = np.array(response_json[\"cosine_similarities\"])\n",
    "        top_documents_indices = response_json[\"top_documents_indices\"]\n",
    "\n",
    "        relevance = np.zeros(len(df))\n",
    "\n",
    "        for pid in query.get('answer_pids', []):\n",
    "            pid_str = str(pid)\n",
    "            indices = np.where(df['pid'] == pid_str)[0]\n",
    "            relevance[indices] = 1\n",
    "\n",
    "        retrievedDocument = cosine_similarities\n",
    "        relevantOrNot = relevance[top_documents_indices]\n",
    "\n",
    "        if relevantOrNot.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        precision, recall = calculate_precision_recall(relevantOrNot, retrievedDocument)\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "        map_score = calculate_map_score(relevantOrNot, retrievedDocument)\n",
    "        all_map_scores.append(map_score)\n",
    "\n",
    "        mrr = calculate_mrr(relevantOrNot)\n",
    "        all_mrrs.append(mrr)\n",
    "\n",
    "    if len(all_precisions) == 0:\n",
    "        print(\"⚠️ No valid queries evaluated. Check PIDs matching and dataset content.\")\n",
    "        return\n",
    "\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_map_score = np.mean(all_map_scores)\n",
    "    avg_mrr = np.mean(all_mrrs)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Evaluation results for dataset: {dataset_path}\")\n",
    "    print(f\"Execution Time (seconds): {elapsed_time:.2f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average MAP Score: {avg_map_score:.4f}\")\n",
    "    print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 8: مثال كامل للاستخدام (تعديل حسب حاجتك)\n",
    "\n",
    "dataset_path = \"lotte/lifestyle/dev/forum\"  \n",
    "\n",
    "def search_function(query, top_n=10):\n",
    "    return search_in_bm25(query, dataset_path, top_k=top_n, initial_k=30)\n",
    "\n",
    "evaluate_search(dataset_path, search_function)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
