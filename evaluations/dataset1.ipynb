{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac328150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for dataset: lotte/lifestyle/dev/forum\n",
      "Execution Time (seconds): 301.16\n",
      "Average Precision: 0.4971\n",
      "Average Recall: 0.4971\n",
      "Average MAP Score: 0.5649\n",
      "Average MRR: 0.5903\n"
     ]
    }
   ],
   "source": [
    "# Ø®Ù„ÙŠØ© 1: Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import json\n",
    "import asyncio\n",
    "import httpx\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#---\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 2: TextProcessor ÙƒÙ…Ø§ Ø³Ø¨Ù‚ (Ù…Ø¹ ØªØ¹Ø¯ÙŠÙ„ Ø¯Ø§Ù„Ø© number_to_words)\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def cleaned_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalization_example(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def stemming_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    def lemmatization_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def number_to_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        converted_words = []\n",
    "        for word in words:\n",
    "            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø£Ø±Ù‚Ø§Ù… Ø¹Ø§Ø¯ÙŠØ© ÙÙ‚Ø·\n",
    "            if word.isdecimal() and word.isascii():\n",
    "                try:\n",
    "                    num = int(word)\n",
    "                    if num <= 999999999999999:\n",
    "                        converted_word = self.inflect_engine.number_to_words(word)\n",
    "                        converted_words.append(converted_word)\n",
    "                    else:\n",
    "                        converted_words.append(\"[Number Out of Range]\")\n",
    "                except (ValueError, inflect.NumOutOfRangeError):\n",
    "                    converted_words.append(\"[Number Out of Range]\")\n",
    "            else:\n",
    "                converted_words.append(word)\n",
    "        return ' '.join(converted_words)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        negated_text = []\n",
    "        negate = False\n",
    "        for word in words:\n",
    "            if word.lower() in ['not', \"n't\"]:\n",
    "                negate = True\n",
    "            elif negate:\n",
    "                negated_text.append(f\"NOT_{word}\")\n",
    "                negate = False\n",
    "            else:\n",
    "                negated_text.append(word)\n",
    "        return ' '.join(negated_text)\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if text is None:\n",
    "            return text\n",
    "        text = self.cleaned_text(text)\n",
    "        text = self.normalization_example(text)\n",
    "        text = self.stemming_example(text)\n",
    "        text = self.lemmatization_example(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.number_to_words(text)\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.normalize_unicode(text)\n",
    "        text = self.handle_negations(text)\n",
    "        text = self.remove_urls(text)\n",
    "        return text\n",
    "\n",
    "processor = TextProcessor()\n",
    "\n",
    "#---\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 3: Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† MongoDB\n",
    "\n",
    "def get_data_from_mongo(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection_name = dataset_path.replace(\"/\", \"_\")\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    pids = []\n",
    "    texts = []\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc and isinstance(doc[\"text\"], str):\n",
    "            pids.append(str(doc[\"doc_id\"]))\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "    df = pd.DataFrame({\"pid\": pids, \"text\": texts})\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    return df\n",
    "\n",
    "#---\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 4: Ø¨Ù†Ø§Ø¡ TF-IDF ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©\n",
    "\n",
    "def build_tfidf_in_memory(df):\n",
    "    vectorizer = TfidfVectorizer(preprocessor=processor.preprocess, max_df=0.5, min_df=1)\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "#---\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 5: Ø§Ù„Ø¨Ø­Ø« ÙÙŠ TF-IDF\n",
    "\n",
    "def search_in_tfidf(query, vectorizer, tfidf_matrix, df, top_n=10):\n",
    "    processed_query = processor.preprocess(query)\n",
    "    query_vector = vectorizer.transform([processed_query])\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix, query_vector).flatten()\n",
    "    top_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    top_docs = df.iloc[top_indices]\n",
    "    results = {\n",
    "        \"top_documents\": top_docs.to_dict(orient=\"records\"),\n",
    "        \"cosine_similarities\": cosine_similarities[top_indices].tolist(),\n",
    "        \"top_documents_indices\": top_indices.tolist()\n",
    "    }\n",
    "    return results\n",
    "\n",
    "#---\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 6: Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_map_scores = []\n",
    "all_mrrs = []\n",
    "\n",
    "def calculate_precision_recall(relevantOrNot, retrievedDocument, threshold=0.5):\n",
    "    binaryResult = (retrievedDocument >= threshold).astype(int)\n",
    "    precision = precision_score(relevantOrNot, binaryResult, average='micro')\n",
    "    recall = recall_score(relevantOrNot, binaryResult, average='micro')\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_map_score(relevantOrNot, retrievedDocument):\n",
    "    return average_precision_score(relevantOrNot, retrievedDocument, average='micro')\n",
    "\n",
    "def calculate_mrr(y_true):\n",
    "    rank_position = np.where(y_true == 1)[0]\n",
    "    if len(rank_position) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (rank_position[0] + 1)\n",
    "\n",
    "def load_queries(queries_paths):\n",
    "    queries = []\n",
    "    for file_path in queries_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    query = json.loads(line.strip())\n",
    "                    if 'query' in query:\n",
    "                        queries.append(query)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "    return queries\n",
    "\n",
    "#---\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 7: ØªÙ‚ÙŠÙŠÙ… Ø¨Ø­Ø« TF-IDF (ÙŠØ±Ø¬Ù‰ ØªØ¹Ø¯ÙŠÙ„ search_function Ø­Ø³Ø¨ Ø­Ø§Ø¬ØªÙƒ)\n",
    "\n",
    "def evaluate_search(dataset_path, search_function):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = get_data_from_mongo(dataset_path)\n",
    "    \n",
    "    queries_paths = ''\n",
    "    if dataset_path == 'lotte/lifestyle/dev/forum':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\qas.search.jsonl'\n",
    "    elif dataset_path == 'antique/train':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\antique\\test\\Answers.jsonl'\n",
    "    else:\n",
    "        print(\"Warning: No queries path configured for this dataset.\")\n",
    "        return\n",
    "    \n",
    "    queries = load_queries([queries_paths])\n",
    "\n",
    "    for query in queries:\n",
    "        if 'query' not in query:\n",
    "            continue\n",
    "        \n",
    "        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø« Ù…Ø¹ Ø§Ù„Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ±Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©\n",
    "        response_json = search_function(query['query'], top_n=10)\n",
    "        \n",
    "        top_documents = response_json[\"top_documents\"]\n",
    "        cosine_similarities = np.array(response_json[\"cosine_similarities\"])\n",
    "        top_documents_indices = response_json[\"top_documents_indices\"]\n",
    "\n",
    "        relevance = np.zeros(len(df))\n",
    "\n",
    "        for pid in query.get('answer_pids', []):\n",
    "            pid_str = str(pid)\n",
    "            indices = np.where(df['pid'] == pid_str)[0]\n",
    "            relevance[indices] = 1\n",
    "\n",
    "        retrievedDocument = cosine_similarities\n",
    "        relevantOrNot = relevance[top_documents_indices]\n",
    "\n",
    "        if relevantOrNot.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        precision, recall = calculate_precision_recall(relevantOrNot, retrievedDocument)\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "        map_score = calculate_map_score(relevantOrNot, retrievedDocument)\n",
    "        all_map_scores.append(map_score)\n",
    "\n",
    "        mrr = calculate_mrr(relevantOrNot)\n",
    "        all_mrrs.append(mrr)\n",
    "\n",
    "    if len(all_precisions) == 0:\n",
    "        print(\"âš ï¸ No valid queries evaluated. Check PIDs matching and dataset content.\")\n",
    "        return\n",
    "\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_map_score = np.mean(all_map_scores)\n",
    "    avg_mrr = np.mean(all_mrrs)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Evaluation results for dataset: {dataset_path}\")\n",
    "    print(f\"Execution Time (seconds): {elapsed_time:.2f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average MAP Score: {avg_map_score:.4f}\")\n",
    "    print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "\n",
    "#---\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 8: Ù…Ø«Ø§Ù„ ÙƒØ§Ù…Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… (ØªØ¹Ø¯ÙŠÙ„ Ø­Ø³Ø¨ Ø­Ø§Ø¬ØªÙƒ)\n",
    "\n",
    "dataset_path = \"lotte/lifestyle/dev/forum\"  # ØºÙŠØ± Ù‡Ø°Ø§ Ø­Ø³Ø¨ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ\n",
    "\n",
    "df = get_data_from_mongo(dataset_path)\n",
    "vectorizer, tfidf_matrix = build_tfidf_in_memory(df)\n",
    "\n",
    "def search_function(query, top_n=10):\n",
    "    return search_in_tfidf(query, vectorizer, tfidf_matrix, df, top_n)\n",
    "\n",
    "# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (ÙÙŠ Ø­Ø§Ù„ Ø£Ø±Ø¯Øª)\n",
    "\n",
    "evaluate_search(dataset_path, search_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d97d58d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for dataset: lotte/lifestyle/dev/forum\n",
      "Execution Time (seconds): 7989.27\n",
      "Average Precision: 0.4691\n",
      "Average Recall: 0.4691\n",
      "Average MAP Score: 0.7245\n",
      "Average MRR: 0.7826\n"
     ]
    }
   ],
   "source": [
    "# Ø®Ù„ÙŠØ© 1: Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#---\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 2: TextProcessor ÙƒÙ…Ø§ Ø³Ø¨Ù‚ (Ù…Ø¹ ØªØ¹Ø¯ÙŠÙ„ Ø¯Ø§Ù„Ø© number_to_words)\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def cleaned_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalization_example(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def stemming_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    def lemmatization_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def number_to_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        converted_words = []\n",
    "        for word in words:\n",
    "            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø£Ø±Ù‚Ø§Ù… Ø¹Ø§Ø¯ÙŠØ© ÙÙ‚Ø·\n",
    "            if word.isdecimal() and word.isascii():\n",
    "                try:\n",
    "                    num = int(word)\n",
    "                    if num <= 999999999999999:\n",
    "                        converted_word = self.inflect_engine.number_to_words(word)\n",
    "                        converted_words.append(converted_word)\n",
    "                    else:\n",
    "                        converted_words.append(\"[Number Out of Range]\")\n",
    "                except (ValueError, inflect.NumOutOfRangeError):\n",
    "                    converted_words.append(\"[Number Out of Range]\")\n",
    "            else:\n",
    "                converted_words.append(word)\n",
    "        return ' '.join(converted_words)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        negated_text = []\n",
    "        negate = False\n",
    "        for word in words:\n",
    "            if word.lower() in ['not', \"n't\"]:\n",
    "                negate = True\n",
    "            elif negate:\n",
    "                negated_text.append(f\"NOT_{word}\")\n",
    "                negate = False\n",
    "            else:\n",
    "                negated_text.append(word)\n",
    "        return ' '.join(negated_text)\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if text is None:\n",
    "            return text\n",
    "        text = self.cleaned_text(text)\n",
    "        text = self.normalization_example(text)\n",
    "        text = self.stemming_example(text)\n",
    "        text = self.lemmatization_example(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.number_to_words(text)\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.normalize_unicode(text)\n",
    "        text = self.handle_negations(text)\n",
    "        text = self.remove_urls(text)\n",
    "        return text\n",
    "\n",
    "processor = TextProcessor()\n",
    "\n",
    "#---\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 3: Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† MongoDB\n",
    "\n",
    "def get_data_from_mongo(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection_name = dataset_path.replace(\"/\", \"_\")\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    pids = []\n",
    "    texts = []\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc and isinstance(doc[\"text\"], str):\n",
    "            pids.append(str(doc[\"doc_id\"]))\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "    df = pd.DataFrame({\"pid\": pids, \"text\": texts})\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    return df\n",
    "\n",
    "#---\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø®Ø§Ø±Ø¬ Ø§Ù„Ø¯Ø§Ù„Ø© Ø­ØªÙ‰ ØªÙƒÙˆÙ† Ù…Ø±Ø© ÙˆØ­Ø¯Ø©\n",
    "retrieval_model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "processor = TextProcessor()\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ FAISS index ÙˆØ§Ù„Ù€ doc_ids\n",
    "def get_faiss_index_and_doc_ids(dataset_path: str):\n",
    "    db_dir = os.path.join(r\"C:\\Users\\USER\\Desktop\\IR_Final_Project\\db\", dataset_path.replace(\"/\", \"__\"))\n",
    "    index = faiss.read_index(os.path.join(db_dir, \"bert_faiss.index\"))\n",
    "    doc_ids = joblib.load(os.path.join(db_dir, \"bert_doc_ids.joblib\"))\n",
    "    return index, doc_ids\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ ÙƒÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙƒÙ…Ø§Ø¨: doc_id â†’ text\n",
    "def load_documents_map(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection = db[dataset_path.replace(\"/\", \"_\")]\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    doc_map = {}\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc:\n",
    "            doc_map[str(doc[\"doc_id\"])] = doc[\"text\"]\n",
    "    return doc_map\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BERT + FAISS\n",
    "def search_in_bert(query, dataset_path, top_k=50, rerank_k=10):\n",
    "    index, doc_ids = get_faiss_index_and_doc_ids(dataset_path)\n",
    "    doc_map = load_documents_map(dataset_path)\n",
    "    doc_id_to_index = {str(doc_id): i for i, doc_id in enumerate(doc_ids)}\n",
    "\n",
    "    query_processed = processor.preprocess(query)\n",
    "    query_vec = retrieval_model.encode(query_processed, normalize_embeddings=True).astype(np.float32).reshape(1, -1)\n",
    "    faiss.normalize_L2(query_vec)\n",
    "\n",
    "    scores, indices = index.search(query_vec, top_k)\n",
    "    top_doc_ids = [doc_ids[i] for i in indices[0]]\n",
    "    top_docs = [(str(doc_id), doc_map.get(str(doc_id), \"\")) for doc_id in top_doc_ids]\n",
    "\n",
    "    filtered_docs = [(doc_id, text) for doc_id, text in top_docs if text.strip()]\n",
    "    pairs = [(query, text) for _, text in filtered_docs]\n",
    "    rerank_scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    ranked = sorted(zip(filtered_docs, rerank_scores), key=lambda x: x[1], reverse=True)\n",
    "    reranked = ranked[:rerank_k]\n",
    "\n",
    "    top_documents = []\n",
    "    cosine_similarities = []\n",
    "    top_documents_indices = []\n",
    "\n",
    "    for (doc_id, text), score in reranked:\n",
    "        top_documents.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"score\": float(score),\n",
    "            \"text\": text\n",
    "        })\n",
    "        cosine_similarities.append(float(score))\n",
    "        top_documents_indices.append(doc_id_to_index.get(doc_id, -1))\n",
    "\n",
    "    return {\n",
    "        \"top_documents\": top_documents,\n",
    "        \"cosine_similarities\": cosine_similarities,\n",
    "        \"top_documents_indices\": top_documents_indices\n",
    "    }\n",
    "\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 6: Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_map_scores = []\n",
    "all_mrrs = []\n",
    "\n",
    "def calculate_precision_recall(relevantOrNot, retrievedDocument, threshold=0.5):\n",
    "    binaryResult = (retrievedDocument >= threshold).astype(int)\n",
    "    precision = precision_score(relevantOrNot, binaryResult, average='micro')\n",
    "    recall = recall_score(relevantOrNot, binaryResult, average='micro')\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_map_score(relevantOrNot, retrievedDocument):\n",
    "    return average_precision_score(relevantOrNot, retrievedDocument, average='micro')\n",
    "\n",
    "def calculate_mrr(y_true):\n",
    "    rank_position = np.where(y_true == 1)[0]\n",
    "    if len(rank_position) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (rank_position[0] + 1)\n",
    "\n",
    "def load_queries(queries_paths):\n",
    "    queries = []\n",
    "    for file_path in queries_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    query = json.loads(line.strip())\n",
    "                    if 'query' in query:\n",
    "                        queries.append(query)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "    return queries\n",
    "\n",
    "#---\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 7: ØªÙ‚ÙŠÙŠÙ… Ø¨Ø­Ø« TF-IDF (ÙŠØ±Ø¬Ù‰ ØªØ¹Ø¯ÙŠÙ„ search_function Ø­Ø³Ø¨ Ø­Ø§Ø¬ØªÙƒ)\n",
    "\n",
    "def evaluate_search(dataset_path, search_function):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = get_data_from_mongo(dataset_path)\n",
    "    \n",
    "    queries_paths = ''\n",
    "    if dataset_path == 'lotte/lifestyle/dev/forum':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\qas.search.jsonl'\n",
    "    elif dataset_path == 'antique/train':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\antique\\test\\Answers.jsonl'\n",
    "    else:\n",
    "        print(\"Warning: No queries path configured for this dataset.\")\n",
    "        return\n",
    "    \n",
    "    queries = load_queries([queries_paths])\n",
    "\n",
    "    for query in queries:\n",
    "        if 'query' not in query:\n",
    "            continue\n",
    "        \n",
    "        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø« Ù…Ø¹ Ø§Ù„Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ±Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©\n",
    "        response_json = search_function(query['query'], top_n=10)\n",
    "        \n",
    "        top_documents = response_json[\"top_documents\"]\n",
    "        cosine_similarities = np.array(response_json[\"cosine_similarities\"])\n",
    "        top_documents_indices = response_json[\"top_documents_indices\"]\n",
    "\n",
    "        relevance = np.zeros(len(df))\n",
    "\n",
    "        for pid in query.get('answer_pids', []):\n",
    "            pid_str = str(pid)\n",
    "            indices = np.where(df['pid'] == pid_str)[0]\n",
    "            relevance[indices] = 1\n",
    "\n",
    "        retrievedDocument = cosine_similarities\n",
    "        relevantOrNot = relevance[top_documents_indices]\n",
    "\n",
    "        if relevantOrNot.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        precision, recall = calculate_precision_recall(relevantOrNot, retrievedDocument)\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "        map_score = calculate_map_score(relevantOrNot, retrievedDocument)\n",
    "        all_map_scores.append(map_score)\n",
    "\n",
    "        mrr = calculate_mrr(relevantOrNot)\n",
    "        all_mrrs.append(mrr)\n",
    "\n",
    "    if len(all_precisions) == 0:\n",
    "        print(\"âš ï¸ No valid queries evaluated. Check PIDs matching and dataset content.\")\n",
    "        return\n",
    "\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_map_score = np.mean(all_map_scores)\n",
    "    avg_mrr = np.mean(all_mrrs)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Evaluation results for dataset: {dataset_path}\")\n",
    "    print(f\"Execution Time (seconds): {elapsed_time:.2f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average MAP Score: {avg_map_score:.4f}\")\n",
    "    print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "\n",
    "#---\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 8: Ù…Ø«Ø§Ù„ ÙƒØ§Ù…Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… (ØªØ¹Ø¯ÙŠÙ„ Ø­Ø³Ø¨ Ø­Ø§Ø¬ØªÙƒ)\n",
    "\n",
    "dataset_path = \"lotte/lifestyle/dev/forum\"  \n",
    "\n",
    "def search_function(query, top_n=10):\n",
    "    return search_in_bert(query, dataset_path, top_k=50, rerank_k=top_n)\n",
    "\n",
    "# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "evaluate_search(dataset_path, search_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70cd64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for dataset: lotte/lifestyle/dev/forum\n",
      "Execution Time (seconds): 1247.31\n",
      "Average Precision: 0.4807\n",
      "Average Recall: 0.4807\n",
      "Average MAP Score: 0.5537\n",
      "Average MRR: 0.5892\n"
     ]
    }
   ],
   "source": [
    "# Ø®Ù„ÙŠØ© 1: Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#---\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 2: TextProcessor ÙƒÙ…Ø§ Ø³Ø¨Ù‚ (Ù…Ø¹ ØªØ¹Ø¯ÙŠÙ„ Ø¯Ø§Ù„Ø© number_to_words)\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def cleaned_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalization_example(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def stemming_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    def lemmatization_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def number_to_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        converted_words = []\n",
    "        for word in words:\n",
    "            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø£Ø±Ù‚Ø§Ù… Ø¹Ø§Ø¯ÙŠØ© ÙÙ‚Ø·\n",
    "            if word.isdecimal() and word.isascii():\n",
    "                try:\n",
    "                    num = int(word)\n",
    "                    if num <= 999999999999999:\n",
    "                        converted_word = self.inflect_engine.number_to_words(word)\n",
    "                        converted_words.append(converted_word)\n",
    "                    else:\n",
    "                        converted_words.append(\"[Number Out of Range]\")\n",
    "                except (ValueError, inflect.NumOutOfRangeError):\n",
    "                    converted_words.append(\"[Number Out of Range]\")\n",
    "            else:\n",
    "                converted_words.append(word)\n",
    "        return ' '.join(converted_words)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        negated_text = []\n",
    "        negate = False\n",
    "        for word in words:\n",
    "            if word.lower() in ['not', \"n't\"]:\n",
    "                negate = True\n",
    "            elif negate:\n",
    "                negated_text.append(f\"NOT_{word}\")\n",
    "                negate = False\n",
    "            else:\n",
    "                negated_text.append(word)\n",
    "        return ' '.join(negated_text)\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if text is None:\n",
    "            return text\n",
    "        text = self.cleaned_text(text)\n",
    "        text = self.normalization_example(text)\n",
    "        text = self.stemming_example(text)\n",
    "        text = self.lemmatization_example(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.number_to_words(text)\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.normalize_unicode(text)\n",
    "        text = self.handle_negations(text)\n",
    "        text = self.remove_urls(text)\n",
    "        return text\n",
    "\n",
    "processor = TextProcessor()\n",
    "\n",
    "#---\n",
    "def build_tfidf_in_memory(df):\n",
    "    vectorizer = TfidfVectorizer(preprocessor=processor.preprocess, max_df=0.5, min_df=1)\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 3: Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† MongoDB\n",
    "\n",
    "def get_data_from_mongo(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection_name = dataset_path.replace(\"/\", \"_\")\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    pids = []\n",
    "    texts = []\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc and isinstance(doc[\"text\"], str):\n",
    "            pids.append(str(doc[\"doc_id\"]))\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "    df = pd.DataFrame({\"pid\": pids, \"text\": texts})\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    return df\n",
    "\n",
    "from functools import lru_cache\n",
    "from sklearn.decomposition import TruncatedSVD  # Ø¥Ø°Ø§ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ Ø¶Ù…Ù† Ù…Ù„Ù joblibØŒ Ù„ÙƒÙ† Ù…Ø§ Ø±Ø­ Ù†Ø¹Ù…Ù„ import Ø¬Ø¯ÙŠØ¯ Ù‡Ù†Ø§ Ø­Ø³Ø¨ Ø·Ù„Ø¨Ùƒ\n",
    "\n",
    "# ÙƒØ§Ø´ Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯Ù„\n",
    "@lru_cache(maxsize=1)\n",
    "def load_bert_model():\n",
    "    return SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "# ÙƒØ§Ø´ Ø¹Ø§Ù… Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø«Ù‚ÙŠÙ„Ø©\n",
    "_loaded_cache = {}\n",
    "\n",
    "def load_cached(path, loader=joblib.load):\n",
    "    if path not in _loaded_cache:\n",
    "        _loaded_cache[path] = loader(path)\n",
    "    return _loaded_cache[path]\n",
    "\n",
    "\n",
    "def search_in_hybrid(query, dataset_path, tfidf_vectorizer, svd,top_n=10, tfidf_weight=0.4, bert_weight=0.6):\n",
    "    db_dir = os.path.join(r\"C:\\Users\\USER\\Desktop\\IR_Final_Project\\db\", dataset_path.replace(\"/\", \"__\"))\n",
    "    docs_df = get_data_from_mongo(dataset_path)\n",
    "\n",
    "    query_processed = processor.preprocess(query)\n",
    "\n",
    "    tfidf_q = tfidf_vectorizer.transform([query_processed])\n",
    "    tfidf_q_reduced = svd.transform(tfidf_q)\n",
    "\n",
    "    model = load_bert_model()\n",
    "    bert_q = model.encode([query_processed], normalize_embeddings=True)\n",
    "\n",
    "    min_dim = min(tfidf_q_reduced.shape[1], bert_q.shape[1])\n",
    "    tfidf_q_reduced_cut = tfidf_q_reduced[:, :min_dim]\n",
    "    bert_q_cut = bert_q[:, :min_dim]\n",
    "\n",
    "    hybrid_query = tfidf_weight * tfidf_q_reduced_cut + bert_weight * bert_q_cut\n",
    "    hybrid_query = np.ascontiguousarray(hybrid_query.astype(np.float32))\n",
    "    faiss.normalize_L2(hybrid_query)\n",
    "\n",
    "    index = faiss.read_index(os.path.join(db_dir, \"hybrid_faiss.index\"))\n",
    "    D, I = index.search(hybrid_query, top_n)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        results.append({\n",
    "            \"doc_id\": docs_df.iloc[idx][\"pid\"],\n",
    "            \"score\": float(score),\n",
    "            \"text\": docs_df.iloc[idx][\"text\"]\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"top_documents\": results,\n",
    "        \"cosine_similarities\": D[0].tolist(),\n",
    "        \"top_documents_indices\": I[0].tolist()\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 6: Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_map_scores = []\n",
    "all_mrrs = []\n",
    "\n",
    "def calculate_precision_recall(relevantOrNot, retrievedDocument, threshold=0.5):\n",
    "    binaryResult = (retrievedDocument >= threshold).astype(int)\n",
    "    precision = precision_score(relevantOrNot, binaryResult, average='micro')\n",
    "    recall = recall_score(relevantOrNot, binaryResult, average='micro')\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_map_score(relevantOrNot, retrievedDocument):\n",
    "    return average_precision_score(relevantOrNot, retrievedDocument, average='micro')\n",
    "\n",
    "def calculate_mrr(y_true):\n",
    "    rank_position = np.where(y_true == 1)[0]\n",
    "    if len(rank_position) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (rank_position[0] + 1)\n",
    "\n",
    "def load_queries(queries_paths):\n",
    "    queries = []\n",
    "    for file_path in queries_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    query = json.loads(line.strip())\n",
    "                    if 'query' in query:\n",
    "                        queries.append(query)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "    return queries\n",
    "\n",
    "#---\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 7: ØªÙ‚ÙŠÙŠÙ… Ø¨Ø­Ø« TF-IDF (ÙŠØ±Ø¬Ù‰ ØªØ¹Ø¯ÙŠÙ„ search_function Ø­Ø³Ø¨ Ø­Ø§Ø¬ØªÙƒ)\n",
    "\n",
    "def evaluate_search(dataset_path, search_function):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = get_data_from_mongo(dataset_path)\n",
    "    \n",
    "    queries_paths = ''\n",
    "    if dataset_path == 'lotte/lifestyle/dev/forum':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\qas.search.jsonl'\n",
    "    elif dataset_path == 'antique/train':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\antique\\test\\Answers.jsonl'\n",
    "    else:\n",
    "        print(\"Warning: No queries path configured for this dataset.\")\n",
    "        return\n",
    "    \n",
    "    queries = load_queries([queries_paths])\n",
    "\n",
    "    for query in queries:\n",
    "        if 'query' not in query:\n",
    "            continue\n",
    "        \n",
    "        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø« Ù…Ø¹ Ø§Ù„Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ±Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©\n",
    "        response_json = search_function(query['query'], top_n=10)\n",
    "        \n",
    "        top_documents = response_json[\"top_documents\"]\n",
    "        cosine_similarities = np.array(response_json[\"cosine_similarities\"])\n",
    "        top_documents_indices = response_json[\"top_documents_indices\"]\n",
    "\n",
    "        relevance = np.zeros(len(df))\n",
    "\n",
    "        for pid in query.get('answer_pids', []):\n",
    "            pid_str = str(pid)\n",
    "            indices = np.where(df['pid'] == pid_str)[0]\n",
    "            relevance[indices] = 1\n",
    "\n",
    "        retrievedDocument = cosine_similarities\n",
    "        relevantOrNot = relevance[top_documents_indices]\n",
    "\n",
    "        if relevantOrNot.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        precision, recall = calculate_precision_recall(relevantOrNot, retrievedDocument)\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "        map_score = calculate_map_score(relevantOrNot, retrievedDocument)\n",
    "        all_map_scores.append(map_score)\n",
    "\n",
    "        mrr = calculate_mrr(relevantOrNot)\n",
    "        all_mrrs.append(mrr)\n",
    "\n",
    "    if len(all_precisions) == 0:\n",
    "        print(\"âš ï¸ No valid queries evaluated. Check PIDs matching and dataset content.\")\n",
    "        return\n",
    "\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_map_score = np.mean(all_map_scores)\n",
    "    avg_mrr = np.mean(all_mrrs)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Evaluation results for dataset: {dataset_path}\")\n",
    "    print(f\"Execution Time (seconds): {elapsed_time:.2f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average MAP Score: {avg_map_score:.4f}\")\n",
    "    print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "\n",
    "#---\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 8: Ù…Ø«Ø§Ù„ ÙƒØ§Ù…Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… (ØªØ¹Ø¯ÙŠÙ„ Ø­Ø³Ø¨ Ø­Ø§Ø¬ØªÙƒ)\n",
    "\n",
    "dataset_path = \"lotte/lifestyle/dev/forum\"  \n",
    "df = get_data_from_mongo(dataset_path)\n",
    "tfidf_vectorizer, tfidf_matrix = build_tfidf_in_memory(df)\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "tfidf_matrix_reduced = svd.fit_transform(tfidf_matrix)\n",
    "def search_function(query, top_n=10):\n",
    "    return search_in_hybrid(query, dataset_path, tfidf_vectorizer, svd, top_n=top_n, tfidf_weight=0.4, bert_weight=0.6)\n",
    "\n",
    "# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "evaluate_search(dataset_path, search_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a2d0bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Creating BM25-style weighted inverted index in memory for dataset: lotte/lifestyle/dev/forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_17848\\628331124.py:38: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_17848\\628331124.py:38: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BM25-style weighted inverted index created and stored in cache for dataset: lotte/lifestyle/dev/forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_17848\\628331124.py:38: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for dataset: lotte/lifestyle/dev/forum\n",
      "Execution Time (seconds): 9489.99\n",
      "Average Precision: 0.5324\n",
      "Average Recall: 0.5324\n",
      "Average MAP Score: 0.7502\n",
      "Average MRR: 0.7833\n"
     ]
    }
   ],
   "source": [
    "# Ø®Ù„ÙŠØ© 1: Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#--- Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…ÙˆÙ†Ø¬Ùˆ ÙˆØ§Ù„ÙƒØ§Ø´\n",
    "\n",
    "def get_mongo_connection():\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    return db\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def bm25_processed_text(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def get_data_from_mongo(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection_name = dataset_path.replace(\"/\", \"_\")\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    pids = []\n",
    "    texts = []\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc and isinstance(doc[\"text\"], str):\n",
    "            pids.append(str(doc[\"doc_id\"]))\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "    df = pd.DataFrame({\"pid\": pids, \"text\": texts})\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    return df\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def load_bert_model():\n",
    "    return SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "_loaded_cache = {}\n",
    "def load_cached(path, loader=joblib.load):\n",
    "    if path not in _loaded_cache:\n",
    "        _loaded_cache[path] = loader(path)\n",
    "    return _loaded_cache[path]\n",
    "\n",
    "bm25_cache = {}\n",
    "\n",
    "def get_bm25_components(dataset_path):\n",
    "    safe_name = os.path.join(r\"C:\\Users\\USER\\Desktop\\IR_Final_Project\\db\", dataset_path.replace(\"/\", \"__\"))\n",
    "    if safe_name not in bm25_cache:\n",
    "        base_path = os.path.join(\"db\", safe_name)\n",
    "        bm25_model = joblib.load(os.path.join(base_path, \"bm25_model.joblib\"))\n",
    "        doc_ids = joblib.load(os.path.join(base_path, \"doc_ids.joblib\"))\n",
    "        tokenized_texts = joblib.load(os.path.join(base_path, \"all_tokenized_texts.joblib\"))\n",
    "        bm25_cache[safe_name] = (bm25_model, doc_ids, tokenized_texts)\n",
    "    return bm25_cache[safe_name]\n",
    "\n",
    "def load_documents_by_ids(dataset_path: str, doc_ids):\n",
    "    db = get_mongo_connection()\n",
    "    collection = db[dataset_path.replace(\"/\", \"_\")]\n",
    "    cursor = collection.find({\"doc_id\": {\"$in\": list(doc_ids)}}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    return {doc[\"doc_id\"]: doc[\"text\"] for doc in cursor if \"text\" in doc}\n",
    "\n",
    "cross_encoder = None\n",
    "def get_cross_encoder():\n",
    "    global cross_encoder\n",
    "    if cross_encoder is None:\n",
    "        cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "    return cross_encoder\n",
    "\n",
    "#--- ÙƒØ§Ø´ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù„Ù€ BM25 weighted inverted index\n",
    "\n",
    "bm25_weighted_index_cache = {}\n",
    "\n",
    "def create_bm25_weighted_inverted_index_in_memory(dataset_path):\n",
    "    db = get_mongo_connection()\n",
    "    collection = db[dataset_path.replace(\"/\", \"_\")]\n",
    "\n",
    "    inverted_index = defaultdict(list)\n",
    "    total_docs = 0\n",
    "\n",
    "    print(f\"ğŸ”„ Creating BM25-style weighted inverted index in memory for dataset: {dataset_path}\")\n",
    "\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc:\n",
    "            total_docs += 1\n",
    "            doc_id = doc[\"doc_id\"]\n",
    "            tokens = bm25_processed_text(doc[\"text\"])\n",
    "            token_freq = defaultdict(int)\n",
    "            for token in tokens:\n",
    "                token_freq[token] += 1\n",
    "            for token, freq in token_freq.items():\n",
    "                inverted_index[token].append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"weight\": freq\n",
    "                })\n",
    "\n",
    "    bm25_weighted_index_cache[dataset_path] = dict(inverted_index)\n",
    "\n",
    "    print(f\"âœ… BM25-style weighted inverted index created and stored in cache for dataset: {dataset_path}\")\n",
    "    return {\n",
    "        \"status\": \"BM25 weighted inverted index created in memory\",\n",
    "        \"terms_count\": len(inverted_index),\n",
    "        \"documents_indexed\": total_docs\n",
    "    }\n",
    "\n",
    "def get_weighted_index(dataset_path):\n",
    "    if dataset_path not in bm25_weighted_index_cache:\n",
    "        create_bm25_weighted_inverted_index_in_memory(dataset_path)\n",
    "    return bm25_weighted_index_cache[dataset_path]\n",
    "\n",
    "def expand_query(tokens):\n",
    "    expanded = set(tokens)\n",
    "    for token in tokens:\n",
    "        for syn in wordnet.synsets(token):\n",
    "            for lemma in syn.lemmas():\n",
    "                expanded.add(lemma.name().replace(\"_\", \" \"))\n",
    "    return list(expanded)\n",
    "\n",
    "def search_in_bm25(query, dataset_path, top_k=10, initial_k=30):\n",
    "\n",
    "    bm25_model, doc_ids, tokenized_texts = get_bm25_components(dataset_path)\n",
    "\n",
    "    query_tokens = bm25_processed_text(query)\n",
    "    if not query_tokens:\n",
    "        return {\"top_documents\": [], \"cosine_similarities\": [], \"top_documents_indices\": []}\n",
    "\n",
    "    expanded_query = expand_query(query_tokens)\n",
    "\n",
    "    weighted_index = get_weighted_index(dataset_path)\n",
    "\n",
    "    candidate_doc_ids = set()\n",
    "    for term in expanded_query:\n",
    "        if term in weighted_index:\n",
    "            for entry in weighted_index[term]:\n",
    "                if isinstance(entry, dict):\n",
    "                    candidate_doc_ids.add(entry[\"doc_id\"])\n",
    "                else:\n",
    "                    candidate_doc_ids.add(entry)\n",
    "\n",
    "    if not candidate_doc_ids:\n",
    "        return {\"top_documents\": [], \"cosine_similarities\": [], \"top_documents_indices\": []}\n",
    "\n",
    "    doc_id_to_index = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
    "    candidate_indices = [doc_id_to_index[doc_id] for doc_id in candidate_doc_ids if doc_id in doc_id_to_index]\n",
    "\n",
    "    if not candidate_indices:\n",
    "        return {\"top_documents\": [], \"cosine_similarities\": [], \"top_documents_indices\": []}\n",
    "\n",
    "    scores = bm25_model.get_scores(expanded_query)\n",
    "    candidate_scores = [(i, scores[i]) for i in candidate_indices]\n",
    "    candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_candidates = candidate_scores[:initial_k]\n",
    "    top_indices = [i for i, _ in top_candidates]\n",
    "    top_doc_ids = [doc_ids[i] for i in top_indices]\n",
    "    doc_texts = load_documents_by_ids(dataset_path, top_doc_ids)\n",
    "\n",
    "    cross_encoder = get_cross_encoder()\n",
    "    cross_inputs = [(query, doc_texts[doc_ids[i]]) for i in top_indices]\n",
    "    rerank_scores = cross_encoder.predict(cross_inputs, batch_size=16)\n",
    "\n",
    "    reranked = sorted(zip(top_doc_ids, rerank_scores, top_indices), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    top_documents = []\n",
    "    cosine_similarities = []\n",
    "    top_documents_indices = []\n",
    "\n",
    "    for doc_id, score, idx in reranked:\n",
    "        top_documents.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"score\": float(score),\n",
    "            \"text\": doc_texts[doc_id]\n",
    "        })\n",
    "        cosine_similarities.append(float(score))\n",
    "        top_documents_indices.append(idx)\n",
    "\n",
    "    return {\n",
    "        \"top_documents\": top_documents,\n",
    "        \"cosine_similarities\": cosine_similarities,\n",
    "        \"top_documents_indices\": top_documents_indices\n",
    "    }\n",
    "\n",
    "#--- Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_map_scores = []\n",
    "all_mrrs = []\n",
    "\n",
    "def calculate_precision_recall(relevantOrNot, retrievedDocument, threshold=0.5):\n",
    "    binaryResult = (retrievedDocument >= threshold).astype(int)\n",
    "    precision = precision_score(relevantOrNot, binaryResult, average='micro')\n",
    "    recall = recall_score(relevantOrNot, binaryResult, average='micro')\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_map_score(relevantOrNot, retrievedDocument):\n",
    "    return average_precision_score(relevantOrNot, retrievedDocument, average='micro')\n",
    "\n",
    "def calculate_mrr(y_true):\n",
    "    rank_position = np.where(y_true == 1)[0]\n",
    "    if len(rank_position) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (rank_position[0] + 1)\n",
    "\n",
    "def load_queries(queries_paths):\n",
    "    queries = []\n",
    "    for file_path in queries_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    query = json.loads(line.strip())\n",
    "                    if 'query' in query:\n",
    "                        queries.append(query)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "    return queries\n",
    "\n",
    "#--- Ù…Ø«Ø§Ù„ ÙƒØ§Ù…Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…\n",
    "\n",
    "dataset_path = \"lotte/lifestyle/dev/forum\"\n",
    "\n",
    "def search_function(query, top_n=10):\n",
    "    return search_in_bm25(query, dataset_path, top_k=top_n, initial_k=30)\n",
    "\n",
    "evaluate_search(dataset_path, search_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e000e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loaded_cache.clear()\n",
    "load_bert_model.cache_clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2850229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù„Ù€ lotte/lifestyle/dev/forum\n",
      "Execution Time: 9178.94 s\n",
      "Avg Precision  : 0.4691\n",
      "Avg Recall     : 0.4691\n",
      "Avg MAP        : 0.7245\n",
      "Avg MRR        : 0.7826\n"
     ]
    }
   ],
   "source": [
    "# Ø®Ù„ÙŠØ© 1: Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©\n",
    "# ----------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup   # Ù…ÙˆØ¬ÙˆØ¯Ø© Ø£ØµÙ„Ø§Ù‹ Ø¥Ù† Ø§Ø­ØªØ¬ØªÙ‡Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹\n",
    "import unicodedata\n",
    "import contractions\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 2: ÙƒÙ„Ø§Ø³ TextProcessor\n",
    "# ----------------------------------------------------------\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def cleaned_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalization_example(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def stemming_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        return ' '.join(self.stemmer.stem(w) for w in words)\n",
    "\n",
    "    def lemmatization_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        return ' '.join(self.lemmatizer.lemmatize(w) for w in words)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        return ' '.join(w for w in words if w.lower() not in self.stop_words)\n",
    "\n",
    "    def number_to_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        converted = []\n",
    "        for w in words:\n",
    "            if w.isdecimal() and w.isascii():\n",
    "                try:\n",
    "                    num = int(w)\n",
    "                    if num <= 999_999_999_999_999:\n",
    "                        converted.append(self.inflect_engine.number_to_words(w))\n",
    "                    else:\n",
    "                        converted.append('[Number Out of Range]')\n",
    "                except (ValueError, inflect.NumOutOfRangeError):\n",
    "                    converted.append('[Number Out of Range]')\n",
    "            else:\n",
    "                converted.append(w)\n",
    "        return ' '.join(converted)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        words, out, negate = self.tokenizer.tokenize(text), [], False\n",
    "        for w in words:\n",
    "            if w.lower() in ['not', \"n't\"]:\n",
    "                negate = True\n",
    "            elif negate:\n",
    "                out.append(f'NOT_{w}')\n",
    "                negate = False\n",
    "            else:\n",
    "                out.append(w)\n",
    "        return ' '.join(out)\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if text is None:\n",
    "            return text\n",
    "        text = self.cleaned_text(text)\n",
    "        text = self.normalization_example(text)\n",
    "        text = self.stemming_example(text)\n",
    "        text = self.lemmatization_example(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.number_to_words(text)\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.normalize_unicode(text)\n",
    "        text = self.handle_negations(text)\n",
    "        text = self.remove_urls(text)\n",
    "        return text\n",
    "\n",
    "processor = TextProcessor()\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 3: Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† MongoDB\n",
    "# ----------------------------------------------------------\n",
    "def get_data_from_mongo(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection = db[dataset_path.replace(\"/\", \"_\")]\n",
    "\n",
    "    pids, texts = [], []\n",
    "    for doc in collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1}):\n",
    "        if \"doc_id\" in doc and \"text\" in doc and isinstance(doc[\"text\"], str):\n",
    "            pids.append(str(doc[\"doc_id\"]))\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "    df = pd.DataFrame({\"pid\": pids, \"text\": texts})\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    return df\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 4: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØªØ¹Ø±ÙŠÙ ÙˆØ¸Ø§Ø¦Ù Ù…Ø³Ø§Ø¹Ø¯Ø©\n",
    "# ----------------------------------------------------------\n",
    "retrieval_model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "cross_encoder   = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "processor       = TextProcessor()\n",
    "\n",
    "def load_documents_map(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection = db[dataset_path.replace(\"/\", \"_\")]\n",
    "    doc_map = {}\n",
    "    for d in collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1}):\n",
    "        if \"doc_id\" in d and \"text\" in d:\n",
    "            doc_map[str(d[\"doc_id\"])] = d[\"text\"]\n",
    "    return doc_map\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 5: search_in_bert (Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø¯ÙˆÙ† FAISS)\n",
    "# ----------------------------------------------------------\n",
    "# ÙƒØ§Ø´ Ù…Ø­Ù„ÙŠ Ù„Ù„Ø¥ÙŠÙ…Ø¨Ø¯Ù†ØºØ² ÙˆØ§Ù„Ù€ doc_ids\n",
    "embedding_cache = {}\n",
    "\n",
    "def load_embeddings_and_doc_ids(dataset_path):\n",
    "    if dataset_path in embedding_cache:\n",
    "        return embedding_cache[dataset_path]\n",
    "\n",
    "    db_dir = os.path.join(r\"C:\\Users\\USER\\Desktop\\IR_Final_Project\\db\",\n",
    "                          dataset_path.replace(\"/\", \"__\"))\n",
    "    embeddings = joblib.load(os.path.join(db_dir, \"bert_embeddings.joblib\"))\n",
    "    doc_ids    = joblib.load(os.path.join(db_dir, \"bert_doc_ids.joblib\"))\n",
    "\n",
    "    embedding_cache[dataset_path] = (embeddings, doc_ids)\n",
    "    return embeddings, doc_ids\n",
    "\n",
    "\n",
    "def search_in_bert(query, dataset_path, top_k=50, rerank_k=10):\n",
    "    embeddings, doc_ids = load_embeddings_and_doc_ids(dataset_path)\n",
    "    doc_map             = load_documents_map(dataset_path)\n",
    "    doc_id_to_index     = {str(d): i for i, d in enumerate(doc_ids)}\n",
    "\n",
    "    query_vec = retrieval_model.encode(\n",
    "        processor.preprocess(query),\n",
    "        normalize_embeddings=True\n",
    "    ).reshape(1, -1)\n",
    "\n",
    "    similarities = cosine_similarity(query_vec, embeddings)[0]        # (N,)\n",
    "    top_idx      = np.argsort(similarities)[::-1][:top_k]\n",
    "    top_doc_ids  = [doc_ids[i] for i in top_idx]\n",
    "    top_docs     = [(str(d), doc_map.get(str(d), \"\")) for d in top_doc_ids]\n",
    "\n",
    "    filtered_docs = [(d, t) for d, t in top_docs if t.strip()]\n",
    "    pairs         = [(query, t) for _, t in filtered_docs]\n",
    "    rerank_scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    ranked   = sorted(zip(filtered_docs, rerank_scores),\n",
    "                      key=lambda x: x[1],\n",
    "                      reverse=True)[:rerank_k]\n",
    "\n",
    "    top_documents, cos_sims, top_doc_indices = [], [], []\n",
    "    for (d, t), s in ranked:\n",
    "        top_documents.append({\"doc_id\": d, \"score\": float(s), \"text\": t})\n",
    "        cos_sims.append(float(s))\n",
    "        top_doc_indices.append(doc_id_to_index.get(d, -1))\n",
    "\n",
    "    return {\n",
    "        \"top_documents\":         top_documents,\n",
    "        \"cosine_similarities\":   cos_sims,\n",
    "        \"top_documents_indices\": top_doc_indices\n",
    "    }\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 6: Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "# ----------------------------------------------------------\n",
    "all_precisions, all_recalls, all_map_scores, all_mrrs = [], [], [], []\n",
    "\n",
    "def calculate_precision_recall(y_true, y_score, thresh=0.5):\n",
    "    bin_res = (y_score >= thresh).astype(int)\n",
    "    return (precision_score(y_true, bin_res, average='micro'),\n",
    "            recall_score(y_true, bin_res, average='micro'))\n",
    "\n",
    "def calculate_map_score(y_true, y_score):\n",
    "    return average_precision_score(y_true, y_score, average='micro')\n",
    "\n",
    "def calculate_mrr(y_true):\n",
    "    pos = np.where(y_true == 1)[0]\n",
    "    return 0 if len(pos) == 0 else 1 / (pos[0] + 1)\n",
    "\n",
    "def load_queries(paths):\n",
    "    out = []\n",
    "    for p in paths:\n",
    "        with open(p, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    q = json.loads(line.strip())\n",
    "                    if 'query' in q: out.append(q)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {p}: {line}\")\n",
    "    return out\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 7: evaluate_search\n",
    "# ----------------------------------------------------------\n",
    "def evaluate_search(dataset_path, search_function):\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    df = get_data_from_mongo(dataset_path)\n",
    "\n",
    "    if dataset_path == 'lotte/lifestyle/dev/forum':\n",
    "        q_path = r'C:\\Users\\USER\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\qas.search.jsonl'\n",
    "    elif dataset_path == 'antique/train':\n",
    "        q_path = r'C:\\Users\\USER\\.ir_datasets\\antique\\test\\Answers.jsonl'\n",
    "    else:\n",
    "        print('âš ï¸ Ù„Ù… ÙŠÙØ¶Ø¨Ø· Ù…Ø³Ø§Ø± Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù„Ù‡Ø°Ø§ Ø§Ù„Ù€ dataset.')\n",
    "        return\n",
    "\n",
    "    queries = load_queries([q_path])\n",
    "\n",
    "    for q in queries:\n",
    "        if 'query' not in q: continue\n",
    "\n",
    "        res  = search_function(q['query'], top_n=10)\n",
    "        sims = np.array(res['cosine_similarities'])\n",
    "        idxs = res['top_documents_indices']\n",
    "        rel  = np.zeros(len(df))\n",
    "        for pid in q.get('answer_pids', []):\n",
    "            rel[np.where(df['pid'] == str(pid))[0]] = 1\n",
    "\n",
    "        y_true = rel[idxs]\n",
    "        if y_true.sum() == 0: continue\n",
    "\n",
    "        p, r = calculate_precision_recall(y_true, sims)\n",
    "        all_precisions.append(p)\n",
    "        all_recalls.append(r)\n",
    "        all_map_scores.append(calculate_map_score(y_true, sims))\n",
    "        all_mrrs.append(calculate_mrr(y_true))\n",
    "\n",
    "    if not all_precisions:\n",
    "        print('âš ï¸ No valid queries evaluated.')\n",
    "        return\n",
    "\n",
    "    print(f'Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù„Ù€ {dataset_path}')\n",
    "    print(f'Execution Time: {time.time() - start:.2f} s')\n",
    "    print(f'Avg Precision  : {np.mean(all_precisions):.4f}')\n",
    "    print(f'Avg Recall     : {np.mean(all_recalls):.4f}')\n",
    "    print(f'Avg MAP        : {np.mean(all_map_scores):.4f}')\n",
    "    print(f'Avg MRR        : {np.mean(all_mrrs):.4f}')\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# Ø®Ù„ÙŠØ© 8: Ù…Ø«Ø§Ù„ ØªØ´ØºÙŠÙ„\n",
    "# ----------------------------------------------------------\n",
    "dataset_path = 'lotte/lifestyle/dev/forum'\n",
    "\n",
    "def search_function(query, top_n=10):\n",
    "    return search_in_bert(query, dataset_path, top_k=50, rerank_k=top_n)\n",
    "\n",
    "# Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:\n",
    "evaluate_search(dataset_path, search_function)\n",
    "# ----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22fba05",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">Note!</span>\n",
    "<span style=\"color:yellow;\">cell evaluation ordering</span>\n",
    "\n",
    "<ol style=\"line-height:1.4\">\n",
    "  <li>tfidf</li>\n",
    "  <li>bert with vector store</li>\n",
    "  <li>hybrid</li>\n",
    "  <li>bm25</li>\n",
    "  <li>bert</li>\n",
    "</ol>\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
