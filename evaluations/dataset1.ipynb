{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac328150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for dataset: lotte/lifestyle/dev/forum\n",
      "Execution Time (seconds): 301.16\n",
      "Average Precision: 0.4971\n",
      "Average Recall: 0.4971\n",
      "Average MAP Score: 0.5649\n",
      "Average MRR: 0.5903\n"
     ]
    }
   ],
   "source": [
    "# خلية 1: استيراد المكتبات اللازمة\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import json\n",
    "import asyncio\n",
    "import httpx\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 2: TextProcessor كما سبق (مع تعديل دالة number_to_words)\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def cleaned_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalization_example(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def stemming_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    def lemmatization_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def number_to_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        converted_words = []\n",
    "        for word in words:\n",
    "            # تحقق من أن الكلمة أرقام عادية فقط\n",
    "            if word.isdecimal() and word.isascii():\n",
    "                try:\n",
    "                    num = int(word)\n",
    "                    if num <= 999999999999999:\n",
    "                        converted_word = self.inflect_engine.number_to_words(word)\n",
    "                        converted_words.append(converted_word)\n",
    "                    else:\n",
    "                        converted_words.append(\"[Number Out of Range]\")\n",
    "                except (ValueError, inflect.NumOutOfRangeError):\n",
    "                    converted_words.append(\"[Number Out of Range]\")\n",
    "            else:\n",
    "                converted_words.append(word)\n",
    "        return ' '.join(converted_words)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        negated_text = []\n",
    "        negate = False\n",
    "        for word in words:\n",
    "            if word.lower() in ['not', \"n't\"]:\n",
    "                negate = True\n",
    "            elif negate:\n",
    "                negated_text.append(f\"NOT_{word}\")\n",
    "                negate = False\n",
    "            else:\n",
    "                negated_text.append(word)\n",
    "        return ' '.join(negated_text)\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if text is None:\n",
    "            return text\n",
    "        text = self.cleaned_text(text)\n",
    "        text = self.normalization_example(text)\n",
    "        text = self.stemming_example(text)\n",
    "        text = self.lemmatization_example(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.number_to_words(text)\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.normalize_unicode(text)\n",
    "        text = self.handle_negations(text)\n",
    "        text = self.remove_urls(text)\n",
    "        return text\n",
    "\n",
    "processor = TextProcessor()\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 3: جلب البيانات من MongoDB\n",
    "\n",
    "def get_data_from_mongo(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection_name = dataset_path.replace(\"/\", \"_\")\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    pids = []\n",
    "    texts = []\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc and isinstance(doc[\"text\"], str):\n",
    "            pids.append(str(doc[\"doc_id\"]))\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "    df = pd.DataFrame({\"pid\": pids, \"text\": texts})\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    return df\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 4: بناء TF-IDF في الذاكرة\n",
    "\n",
    "def build_tfidf_in_memory(df):\n",
    "    vectorizer = TfidfVectorizer(preprocessor=processor.preprocess, max_df=0.5, min_df=1)\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 5: البحث في TF-IDF\n",
    "\n",
    "def search_in_tfidf(query, vectorizer, tfidf_matrix, df, top_n=10):\n",
    "    processed_query = processor.preprocess(query)\n",
    "    query_vector = vectorizer.transform([processed_query])\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix, query_vector).flatten()\n",
    "    top_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    top_docs = df.iloc[top_indices]\n",
    "    results = {\n",
    "        \"top_documents\": top_docs.to_dict(orient=\"records\"),\n",
    "        \"cosine_similarities\": cosine_similarities[top_indices].tolist(),\n",
    "        \"top_documents_indices\": top_indices.tolist()\n",
    "    }\n",
    "    return results\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 6: دوال التقييم\n",
    "\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_map_scores = []\n",
    "all_mrrs = []\n",
    "\n",
    "def calculate_precision_recall(relevantOrNot, retrievedDocument, threshold=0.5):\n",
    "    binaryResult = (retrievedDocument >= threshold).astype(int)\n",
    "    precision = precision_score(relevantOrNot, binaryResult, average='micro')\n",
    "    recall = recall_score(relevantOrNot, binaryResult, average='micro')\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_map_score(relevantOrNot, retrievedDocument):\n",
    "    return average_precision_score(relevantOrNot, retrievedDocument, average='micro')\n",
    "\n",
    "def calculate_mrr(y_true):\n",
    "    rank_position = np.where(y_true == 1)[0]\n",
    "    if len(rank_position) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (rank_position[0] + 1)\n",
    "\n",
    "def load_queries(queries_paths):\n",
    "    queries = []\n",
    "    for file_path in queries_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    query = json.loads(line.strip())\n",
    "                    if 'query' in query:\n",
    "                        queries.append(query)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "    return queries\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 7: تقييم بحث TF-IDF (يرجى تعديل search_function حسب حاجتك)\n",
    "\n",
    "def evaluate_search(dataset_path, search_function):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = get_data_from_mongo(dataset_path)\n",
    "    \n",
    "    queries_paths = ''\n",
    "    if dataset_path == 'lotte/lifestyle/dev/forum':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\qas.search.jsonl'\n",
    "    elif dataset_path == 'antique/train':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\antique\\test\\Answers.jsonl'\n",
    "    else:\n",
    "        print(\"Warning: No queries path configured for this dataset.\")\n",
    "        return\n",
    "    \n",
    "    queries = load_queries([queries_paths])\n",
    "\n",
    "    for query in queries:\n",
    "        if 'query' not in query:\n",
    "            continue\n",
    "        \n",
    "        # استدعاء دالة البحث مع الباراميترات المناسبة\n",
    "        response_json = search_function(query['query'], top_n=10)\n",
    "        \n",
    "        top_documents = response_json[\"top_documents\"]\n",
    "        cosine_similarities = np.array(response_json[\"cosine_similarities\"])\n",
    "        top_documents_indices = response_json[\"top_documents_indices\"]\n",
    "\n",
    "        relevance = np.zeros(len(df))\n",
    "\n",
    "        for pid in query.get('answer_pids', []):\n",
    "            pid_str = str(pid)\n",
    "            indices = np.where(df['pid'] == pid_str)[0]\n",
    "            relevance[indices] = 1\n",
    "\n",
    "        retrievedDocument = cosine_similarities\n",
    "        relevantOrNot = relevance[top_documents_indices]\n",
    "\n",
    "        if relevantOrNot.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        precision, recall = calculate_precision_recall(relevantOrNot, retrievedDocument)\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "        map_score = calculate_map_score(relevantOrNot, retrievedDocument)\n",
    "        all_map_scores.append(map_score)\n",
    "\n",
    "        mrr = calculate_mrr(relevantOrNot)\n",
    "        all_mrrs.append(mrr)\n",
    "\n",
    "    if len(all_precisions) == 0:\n",
    "        print(\"⚠️ No valid queries evaluated. Check PIDs matching and dataset content.\")\n",
    "        return\n",
    "\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_map_score = np.mean(all_map_scores)\n",
    "    avg_mrr = np.mean(all_mrrs)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Evaluation results for dataset: {dataset_path}\")\n",
    "    print(f\"Execution Time (seconds): {elapsed_time:.2f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average MAP Score: {avg_map_score:.4f}\")\n",
    "    print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 8: مثال كامل للاستخدام (تعديل حسب حاجتك)\n",
    "\n",
    "dataset_path = \"lotte/lifestyle/dev/forum\"  # غير هذا حسب بياناتك\n",
    "\n",
    "df = get_data_from_mongo(dataset_path)\n",
    "vectorizer, tfidf_matrix = build_tfidf_in_memory(df)\n",
    "\n",
    "def search_function(query, top_n=10):\n",
    "    return search_in_tfidf(query, vectorizer, tfidf_matrix, df, top_n)\n",
    "\n",
    "# تشغيل التقييم (في حال أردت)\n",
    "\n",
    "evaluate_search(dataset_path, search_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d97d58d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for dataset: lotte/lifestyle/dev/forum\n",
      "Execution Time (seconds): 7989.27\n",
      "Average Precision: 0.4691\n",
      "Average Recall: 0.4691\n",
      "Average MAP Score: 0.7245\n",
      "Average MRR: 0.7826\n"
     ]
    }
   ],
   "source": [
    "# خلية 1: استيراد المكتبات اللازمة\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 2: TextProcessor كما سبق (مع تعديل دالة number_to_words)\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def cleaned_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalization_example(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def stemming_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    def lemmatization_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def number_to_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        converted_words = []\n",
    "        for word in words:\n",
    "            # تحقق من أن الكلمة أرقام عادية فقط\n",
    "            if word.isdecimal() and word.isascii():\n",
    "                try:\n",
    "                    num = int(word)\n",
    "                    if num <= 999999999999999:\n",
    "                        converted_word = self.inflect_engine.number_to_words(word)\n",
    "                        converted_words.append(converted_word)\n",
    "                    else:\n",
    "                        converted_words.append(\"[Number Out of Range]\")\n",
    "                except (ValueError, inflect.NumOutOfRangeError):\n",
    "                    converted_words.append(\"[Number Out of Range]\")\n",
    "            else:\n",
    "                converted_words.append(word)\n",
    "        return ' '.join(converted_words)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        negated_text = []\n",
    "        negate = False\n",
    "        for word in words:\n",
    "            if word.lower() in ['not', \"n't\"]:\n",
    "                negate = True\n",
    "            elif negate:\n",
    "                negated_text.append(f\"NOT_{word}\")\n",
    "                negate = False\n",
    "            else:\n",
    "                negated_text.append(word)\n",
    "        return ' '.join(negated_text)\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if text is None:\n",
    "            return text\n",
    "        text = self.cleaned_text(text)\n",
    "        text = self.normalization_example(text)\n",
    "        text = self.stemming_example(text)\n",
    "        text = self.lemmatization_example(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.number_to_words(text)\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.normalize_unicode(text)\n",
    "        text = self.handle_negations(text)\n",
    "        text = self.remove_urls(text)\n",
    "        return text\n",
    "\n",
    "processor = TextProcessor()\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 3: جلب البيانات من MongoDB\n",
    "\n",
    "def get_data_from_mongo(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection_name = dataset_path.replace(\"/\", \"_\")\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    pids = []\n",
    "    texts = []\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc and isinstance(doc[\"text\"], str):\n",
    "            pids.append(str(doc[\"doc_id\"]))\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "    df = pd.DataFrame({\"pid\": pids, \"text\": texts})\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    return df\n",
    "\n",
    "#---\n",
    "\n",
    "# تحميل الموديلات خارج الدالة حتى تكون مرة وحدة\n",
    "retrieval_model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "processor = TextProcessor()\n",
    "\n",
    "# تحميل FAISS index والـ doc_ids\n",
    "def get_faiss_index_and_doc_ids(dataset_path: str):\n",
    "    db_dir = os.path.join(r\"C:\\Users\\USER\\Desktop\\IR_Final_Project\\db\", dataset_path.replace(\"/\", \"__\"))\n",
    "    index = faiss.read_index(os.path.join(db_dir, \"bert_faiss.index\"))\n",
    "    doc_ids = joblib.load(os.path.join(db_dir, \"bert_doc_ids.joblib\"))\n",
    "    return index, doc_ids\n",
    "\n",
    "# تحميل كل الوثائق كماب: doc_id → text\n",
    "def load_documents_map(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection = db[dataset_path.replace(\"/\", \"_\")]\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    doc_map = {}\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc:\n",
    "            doc_map[str(doc[\"doc_id\"])] = doc[\"text\"]\n",
    "    return doc_map\n",
    "\n",
    "# دالة البحث باستخدام BERT + FAISS\n",
    "def search_in_bert(query, dataset_path, top_k=50, rerank_k=10):\n",
    "    index, doc_ids = get_faiss_index_and_doc_ids(dataset_path)\n",
    "    doc_map = load_documents_map(dataset_path)\n",
    "    doc_id_to_index = {str(doc_id): i for i, doc_id in enumerate(doc_ids)}\n",
    "\n",
    "    query_processed = processor.preprocess(query)\n",
    "    query_vec = retrieval_model.encode(query_processed, normalize_embeddings=True).astype(np.float32).reshape(1, -1)\n",
    "    faiss.normalize_L2(query_vec)\n",
    "\n",
    "    scores, indices = index.search(query_vec, top_k)\n",
    "    top_doc_ids = [doc_ids[i] for i in indices[0]]\n",
    "    top_docs = [(str(doc_id), doc_map.get(str(doc_id), \"\")) for doc_id in top_doc_ids]\n",
    "\n",
    "    filtered_docs = [(doc_id, text) for doc_id, text in top_docs if text.strip()]\n",
    "    pairs = [(query, text) for _, text in filtered_docs]\n",
    "    rerank_scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    ranked = sorted(zip(filtered_docs, rerank_scores), key=lambda x: x[1], reverse=True)\n",
    "    reranked = ranked[:rerank_k]\n",
    "\n",
    "    top_documents = []\n",
    "    cosine_similarities = []\n",
    "    top_documents_indices = []\n",
    "\n",
    "    for (doc_id, text), score in reranked:\n",
    "        top_documents.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"score\": float(score),\n",
    "            \"text\": text\n",
    "        })\n",
    "        cosine_similarities.append(float(score))\n",
    "        top_documents_indices.append(doc_id_to_index.get(doc_id, -1))\n",
    "\n",
    "    return {\n",
    "        \"top_documents\": top_documents,\n",
    "        \"cosine_similarities\": cosine_similarities,\n",
    "        \"top_documents_indices\": top_documents_indices\n",
    "    }\n",
    "\n",
    "\n",
    "# خلية 6: دوال التقييم\n",
    "\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_map_scores = []\n",
    "all_mrrs = []\n",
    "\n",
    "def calculate_precision_recall(relevantOrNot, retrievedDocument, threshold=0.5):\n",
    "    binaryResult = (retrievedDocument >= threshold).astype(int)\n",
    "    precision = precision_score(relevantOrNot, binaryResult, average='micro')\n",
    "    recall = recall_score(relevantOrNot, binaryResult, average='micro')\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_map_score(relevantOrNot, retrievedDocument):\n",
    "    return average_precision_score(relevantOrNot, retrievedDocument, average='micro')\n",
    "\n",
    "def calculate_mrr(y_true):\n",
    "    rank_position = np.where(y_true == 1)[0]\n",
    "    if len(rank_position) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (rank_position[0] + 1)\n",
    "\n",
    "def load_queries(queries_paths):\n",
    "    queries = []\n",
    "    for file_path in queries_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    query = json.loads(line.strip())\n",
    "                    if 'query' in query:\n",
    "                        queries.append(query)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "    return queries\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 7: تقييم بحث TF-IDF (يرجى تعديل search_function حسب حاجتك)\n",
    "\n",
    "def evaluate_search(dataset_path, search_function):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = get_data_from_mongo(dataset_path)\n",
    "    \n",
    "    queries_paths = ''\n",
    "    if dataset_path == 'lotte/lifestyle/dev/forum':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\qas.search.jsonl'\n",
    "    elif dataset_path == 'antique/train':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\antique\\test\\Answers.jsonl'\n",
    "    else:\n",
    "        print(\"Warning: No queries path configured for this dataset.\")\n",
    "        return\n",
    "    \n",
    "    queries = load_queries([queries_paths])\n",
    "\n",
    "    for query in queries:\n",
    "        if 'query' not in query:\n",
    "            continue\n",
    "        \n",
    "        # استدعاء دالة البحث مع الباراميترات المناسبة\n",
    "        response_json = search_function(query['query'], top_n=10)\n",
    "        \n",
    "        top_documents = response_json[\"top_documents\"]\n",
    "        cosine_similarities = np.array(response_json[\"cosine_similarities\"])\n",
    "        top_documents_indices = response_json[\"top_documents_indices\"]\n",
    "\n",
    "        relevance = np.zeros(len(df))\n",
    "\n",
    "        for pid in query.get('answer_pids', []):\n",
    "            pid_str = str(pid)\n",
    "            indices = np.where(df['pid'] == pid_str)[0]\n",
    "            relevance[indices] = 1\n",
    "\n",
    "        retrievedDocument = cosine_similarities\n",
    "        relevantOrNot = relevance[top_documents_indices]\n",
    "\n",
    "        if relevantOrNot.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        precision, recall = calculate_precision_recall(relevantOrNot, retrievedDocument)\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "        map_score = calculate_map_score(relevantOrNot, retrievedDocument)\n",
    "        all_map_scores.append(map_score)\n",
    "\n",
    "        mrr = calculate_mrr(relevantOrNot)\n",
    "        all_mrrs.append(mrr)\n",
    "\n",
    "    if len(all_precisions) == 0:\n",
    "        print(\"⚠️ No valid queries evaluated. Check PIDs matching and dataset content.\")\n",
    "        return\n",
    "\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_map_score = np.mean(all_map_scores)\n",
    "    avg_mrr = np.mean(all_mrrs)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Evaluation results for dataset: {dataset_path}\")\n",
    "    print(f\"Execution Time (seconds): {elapsed_time:.2f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average MAP Score: {avg_map_score:.4f}\")\n",
    "    print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 8: مثال كامل للاستخدام (تعديل حسب حاجتك)\n",
    "\n",
    "dataset_path = \"lotte/lifestyle/dev/forum\"  \n",
    "\n",
    "def search_function(query, top_n=10):\n",
    "    return search_in_bert(query, dataset_path, top_k=50, rerank_k=top_n)\n",
    "\n",
    "# تشغيل التقييم\n",
    "evaluate_search(dataset_path, search_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70cd64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for dataset: lotte/lifestyle/dev/forum\n",
      "Execution Time (seconds): 1247.31\n",
      "Average Precision: 0.4807\n",
      "Average Recall: 0.4807\n",
      "Average MAP Score: 0.5537\n",
      "Average MRR: 0.5892\n"
     ]
    }
   ],
   "source": [
    "# خلية 1: استيراد المكتبات اللازمة\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 2: TextProcessor كما سبق (مع تعديل دالة number_to_words)\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def cleaned_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalization_example(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def stemming_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    def lemmatization_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def number_to_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        converted_words = []\n",
    "        for word in words:\n",
    "            # تحقق من أن الكلمة أرقام عادية فقط\n",
    "            if word.isdecimal() and word.isascii():\n",
    "                try:\n",
    "                    num = int(word)\n",
    "                    if num <= 999999999999999:\n",
    "                        converted_word = self.inflect_engine.number_to_words(word)\n",
    "                        converted_words.append(converted_word)\n",
    "                    else:\n",
    "                        converted_words.append(\"[Number Out of Range]\")\n",
    "                except (ValueError, inflect.NumOutOfRangeError):\n",
    "                    converted_words.append(\"[Number Out of Range]\")\n",
    "            else:\n",
    "                converted_words.append(word)\n",
    "        return ' '.join(converted_words)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        negated_text = []\n",
    "        negate = False\n",
    "        for word in words:\n",
    "            if word.lower() in ['not', \"n't\"]:\n",
    "                negate = True\n",
    "            elif negate:\n",
    "                negated_text.append(f\"NOT_{word}\")\n",
    "                negate = False\n",
    "            else:\n",
    "                negated_text.append(word)\n",
    "        return ' '.join(negated_text)\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if text is None:\n",
    "            return text\n",
    "        text = self.cleaned_text(text)\n",
    "        text = self.normalization_example(text)\n",
    "        text = self.stemming_example(text)\n",
    "        text = self.lemmatization_example(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.number_to_words(text)\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.normalize_unicode(text)\n",
    "        text = self.handle_negations(text)\n",
    "        text = self.remove_urls(text)\n",
    "        return text\n",
    "\n",
    "processor = TextProcessor()\n",
    "\n",
    "#---\n",
    "def build_tfidf_in_memory(df):\n",
    "    vectorizer = TfidfVectorizer(preprocessor=processor.preprocess, max_df=0.5, min_df=1)\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "\n",
    "# خلية 3: جلب البيانات من MongoDB\n",
    "\n",
    "def get_data_from_mongo(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection_name = dataset_path.replace(\"/\", \"_\")\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    pids = []\n",
    "    texts = []\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc and isinstance(doc[\"text\"], str):\n",
    "            pids.append(str(doc[\"doc_id\"]))\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "    df = pd.DataFrame({\"pid\": pids, \"text\": texts})\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    return df\n",
    "\n",
    "from functools import lru_cache\n",
    "from sklearn.decomposition import TruncatedSVD  # إذا مش موجود ضمن ملف joblib، لكن ما رح نعمل import جديد هنا حسب طلبك\n",
    "\n",
    "# كاش لتحميل المودل\n",
    "@lru_cache(maxsize=1)\n",
    "def load_bert_model():\n",
    "    return SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "# كاش عام للملفات الثقيلة\n",
    "_loaded_cache = {}\n",
    "\n",
    "def load_cached(path, loader=joblib.load):\n",
    "    if path not in _loaded_cache:\n",
    "        _loaded_cache[path] = loader(path)\n",
    "    return _loaded_cache[path]\n",
    "\n",
    "\n",
    "def search_in_hybrid(query, dataset_path, tfidf_vectorizer, svd,top_n=10, tfidf_weight=0.4, bert_weight=0.6):\n",
    "    db_dir = os.path.join(r\"C:\\Users\\USER\\Desktop\\IR_Final_Project\\db\", dataset_path.replace(\"/\", \"__\"))\n",
    "    docs_df = get_data_from_mongo(dataset_path)\n",
    "\n",
    "    query_processed = processor.preprocess(query)\n",
    "\n",
    "    tfidf_q = tfidf_vectorizer.transform([query_processed])\n",
    "    tfidf_q_reduced = svd.transform(tfidf_q)\n",
    "\n",
    "    model = load_bert_model()\n",
    "    bert_q = model.encode([query_processed], normalize_embeddings=True)\n",
    "\n",
    "    min_dim = min(tfidf_q_reduced.shape[1], bert_q.shape[1])\n",
    "    tfidf_q_reduced_cut = tfidf_q_reduced[:, :min_dim]\n",
    "    bert_q_cut = bert_q[:, :min_dim]\n",
    "\n",
    "    hybrid_query = tfidf_weight * tfidf_q_reduced_cut + bert_weight * bert_q_cut\n",
    "    hybrid_query = np.ascontiguousarray(hybrid_query.astype(np.float32))\n",
    "    faiss.normalize_L2(hybrid_query)\n",
    "\n",
    "    index = faiss.read_index(os.path.join(db_dir, \"hybrid_faiss.index\"))\n",
    "    D, I = index.search(hybrid_query, top_n)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        results.append({\n",
    "            \"doc_id\": docs_df.iloc[idx][\"pid\"],\n",
    "            \"score\": float(score),\n",
    "            \"text\": docs_df.iloc[idx][\"text\"]\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"top_documents\": results,\n",
    "        \"cosine_similarities\": D[0].tolist(),\n",
    "        \"top_documents_indices\": I[0].tolist()\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# خلية 6: دوال التقييم\n",
    "\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_map_scores = []\n",
    "all_mrrs = []\n",
    "\n",
    "def calculate_precision_recall(relevantOrNot, retrievedDocument, threshold=0.5):\n",
    "    binaryResult = (retrievedDocument >= threshold).astype(int)\n",
    "    precision = precision_score(relevantOrNot, binaryResult, average='micro')\n",
    "    recall = recall_score(relevantOrNot, binaryResult, average='micro')\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_map_score(relevantOrNot, retrievedDocument):\n",
    "    return average_precision_score(relevantOrNot, retrievedDocument, average='micro')\n",
    "\n",
    "def calculate_mrr(y_true):\n",
    "    rank_position = np.where(y_true == 1)[0]\n",
    "    if len(rank_position) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (rank_position[0] + 1)\n",
    "\n",
    "def load_queries(queries_paths):\n",
    "    queries = []\n",
    "    for file_path in queries_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    query = json.loads(line.strip())\n",
    "                    if 'query' in query:\n",
    "                        queries.append(query)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "    return queries\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 7: تقييم بحث TF-IDF (يرجى تعديل search_function حسب حاجتك)\n",
    "\n",
    "def evaluate_search(dataset_path, search_function):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = get_data_from_mongo(dataset_path)\n",
    "    \n",
    "    queries_paths = ''\n",
    "    if dataset_path == 'lotte/lifestyle/dev/forum':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\qas.search.jsonl'\n",
    "    elif dataset_path == 'antique/train':\n",
    "        queries_paths = r'C:\\Users\\USER\\.ir_datasets\\antique\\test\\Answers.jsonl'\n",
    "    else:\n",
    "        print(\"Warning: No queries path configured for this dataset.\")\n",
    "        return\n",
    "    \n",
    "    queries = load_queries([queries_paths])\n",
    "\n",
    "    for query in queries:\n",
    "        if 'query' not in query:\n",
    "            continue\n",
    "        \n",
    "        # استدعاء دالة البحث مع الباراميترات المناسبة\n",
    "        response_json = search_function(query['query'], top_n=10)\n",
    "        \n",
    "        top_documents = response_json[\"top_documents\"]\n",
    "        cosine_similarities = np.array(response_json[\"cosine_similarities\"])\n",
    "        top_documents_indices = response_json[\"top_documents_indices\"]\n",
    "\n",
    "        relevance = np.zeros(len(df))\n",
    "\n",
    "        for pid in query.get('answer_pids', []):\n",
    "            pid_str = str(pid)\n",
    "            indices = np.where(df['pid'] == pid_str)[0]\n",
    "            relevance[indices] = 1\n",
    "\n",
    "        retrievedDocument = cosine_similarities\n",
    "        relevantOrNot = relevance[top_documents_indices]\n",
    "\n",
    "        if relevantOrNot.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        precision, recall = calculate_precision_recall(relevantOrNot, retrievedDocument)\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "        map_score = calculate_map_score(relevantOrNot, retrievedDocument)\n",
    "        all_map_scores.append(map_score)\n",
    "\n",
    "        mrr = calculate_mrr(relevantOrNot)\n",
    "        all_mrrs.append(mrr)\n",
    "\n",
    "    if len(all_precisions) == 0:\n",
    "        print(\"⚠️ No valid queries evaluated. Check PIDs matching and dataset content.\")\n",
    "        return\n",
    "\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_map_score = np.mean(all_map_scores)\n",
    "    avg_mrr = np.mean(all_mrrs)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Evaluation results for dataset: {dataset_path}\")\n",
    "    print(f\"Execution Time (seconds): {elapsed_time:.2f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average MAP Score: {avg_map_score:.4f}\")\n",
    "    print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "\n",
    "#---\n",
    "\n",
    "# خلية 8: مثال كامل للاستخدام (تعديل حسب حاجتك)\n",
    "\n",
    "dataset_path = \"lotte/lifestyle/dev/forum\"  \n",
    "df = get_data_from_mongo(dataset_path)\n",
    "tfidf_vectorizer, tfidf_matrix = build_tfidf_in_memory(df)\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "tfidf_matrix_reduced = svd.fit_transform(tfidf_matrix)\n",
    "def search_function(query, top_n=10):\n",
    "    return search_in_hybrid(query, dataset_path, tfidf_vectorizer, svd, top_n=top_n, tfidf_weight=0.4, bert_weight=0.6)\n",
    "\n",
    "# تشغيل التقييم\n",
    "evaluate_search(dataset_path, search_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a2d0bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating BM25-style weighted inverted index in memory for dataset: lotte/lifestyle/dev/forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_17848\\628331124.py:38: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_17848\\628331124.py:38: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BM25-style weighted inverted index created and stored in cache for dataset: lotte/lifestyle/dev/forum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_17848\\628331124.py:38: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for dataset: lotte/lifestyle/dev/forum\n",
      "Execution Time (seconds): 9489.99\n",
      "Average Precision: 0.5324\n",
      "Average Recall: 0.5324\n",
      "Average MAP Score: 0.7502\n",
      "Average MRR: 0.7833\n"
     ]
    }
   ],
   "source": [
    "# خلية 1: استيراد المكتبات اللازمة\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#--- دوال المونجو والكاش\n",
    "\n",
    "def get_mongo_connection():\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    return db\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def bm25_processed_text(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def get_data_from_mongo(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection_name = dataset_path.replace(\"/\", \"_\")\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    pids = []\n",
    "    texts = []\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc and isinstance(doc[\"text\"], str):\n",
    "            pids.append(str(doc[\"doc_id\"]))\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "    df = pd.DataFrame({\"pid\": pids, \"text\": texts})\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    return df\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def load_bert_model():\n",
    "    return SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "_loaded_cache = {}\n",
    "def load_cached(path, loader=joblib.load):\n",
    "    if path not in _loaded_cache:\n",
    "        _loaded_cache[path] = loader(path)\n",
    "    return _loaded_cache[path]\n",
    "\n",
    "bm25_cache = {}\n",
    "\n",
    "def get_bm25_components(dataset_path):\n",
    "    safe_name = os.path.join(r\"C:\\Users\\USER\\Desktop\\IR_Final_Project\\db\", dataset_path.replace(\"/\", \"__\"))\n",
    "    if safe_name not in bm25_cache:\n",
    "        base_path = os.path.join(\"db\", safe_name)\n",
    "        bm25_model = joblib.load(os.path.join(base_path, \"bm25_model.joblib\"))\n",
    "        doc_ids = joblib.load(os.path.join(base_path, \"doc_ids.joblib\"))\n",
    "        tokenized_texts = joblib.load(os.path.join(base_path, \"all_tokenized_texts.joblib\"))\n",
    "        bm25_cache[safe_name] = (bm25_model, doc_ids, tokenized_texts)\n",
    "    return bm25_cache[safe_name]\n",
    "\n",
    "def load_documents_by_ids(dataset_path: str, doc_ids):\n",
    "    db = get_mongo_connection()\n",
    "    collection = db[dataset_path.replace(\"/\", \"_\")]\n",
    "    cursor = collection.find({\"doc_id\": {\"$in\": list(doc_ids)}}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "    return {doc[\"doc_id\"]: doc[\"text\"] for doc in cursor if \"text\" in doc}\n",
    "\n",
    "cross_encoder = None\n",
    "def get_cross_encoder():\n",
    "    global cross_encoder\n",
    "    if cross_encoder is None:\n",
    "        cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "    return cross_encoder\n",
    "\n",
    "#--- كاش في الذاكرة للـ BM25 weighted inverted index\n",
    "\n",
    "bm25_weighted_index_cache = {}\n",
    "\n",
    "def create_bm25_weighted_inverted_index_in_memory(dataset_path):\n",
    "    db = get_mongo_connection()\n",
    "    collection = db[dataset_path.replace(\"/\", \"_\")]\n",
    "\n",
    "    inverted_index = defaultdict(list)\n",
    "    total_docs = 0\n",
    "\n",
    "    print(f\"🔄 Creating BM25-style weighted inverted index in memory for dataset: {dataset_path}\")\n",
    "\n",
    "    cursor = collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1})\n",
    "\n",
    "    for doc in cursor:\n",
    "        if \"doc_id\" in doc and \"text\" in doc:\n",
    "            total_docs += 1\n",
    "            doc_id = doc[\"doc_id\"]\n",
    "            tokens = bm25_processed_text(doc[\"text\"])\n",
    "            token_freq = defaultdict(int)\n",
    "            for token in tokens:\n",
    "                token_freq[token] += 1\n",
    "            for token, freq in token_freq.items():\n",
    "                inverted_index[token].append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"weight\": freq\n",
    "                })\n",
    "\n",
    "    bm25_weighted_index_cache[dataset_path] = dict(inverted_index)\n",
    "\n",
    "    print(f\"✅ BM25-style weighted inverted index created and stored in cache for dataset: {dataset_path}\")\n",
    "    return {\n",
    "        \"status\": \"BM25 weighted inverted index created in memory\",\n",
    "        \"terms_count\": len(inverted_index),\n",
    "        \"documents_indexed\": total_docs\n",
    "    }\n",
    "\n",
    "def get_weighted_index(dataset_path):\n",
    "    if dataset_path not in bm25_weighted_index_cache:\n",
    "        create_bm25_weighted_inverted_index_in_memory(dataset_path)\n",
    "    return bm25_weighted_index_cache[dataset_path]\n",
    "\n",
    "def expand_query(tokens):\n",
    "    expanded = set(tokens)\n",
    "    for token in tokens:\n",
    "        for syn in wordnet.synsets(token):\n",
    "            for lemma in syn.lemmas():\n",
    "                expanded.add(lemma.name().replace(\"_\", \" \"))\n",
    "    return list(expanded)\n",
    "\n",
    "def search_in_bm25(query, dataset_path, top_k=10, initial_k=30):\n",
    "\n",
    "    bm25_model, doc_ids, tokenized_texts = get_bm25_components(dataset_path)\n",
    "\n",
    "    query_tokens = bm25_processed_text(query)\n",
    "    if not query_tokens:\n",
    "        return {\"top_documents\": [], \"cosine_similarities\": [], \"top_documents_indices\": []}\n",
    "\n",
    "    expanded_query = expand_query(query_tokens)\n",
    "\n",
    "    weighted_index = get_weighted_index(dataset_path)\n",
    "\n",
    "    candidate_doc_ids = set()\n",
    "    for term in expanded_query:\n",
    "        if term in weighted_index:\n",
    "            for entry in weighted_index[term]:\n",
    "                if isinstance(entry, dict):\n",
    "                    candidate_doc_ids.add(entry[\"doc_id\"])\n",
    "                else:\n",
    "                    candidate_doc_ids.add(entry)\n",
    "\n",
    "    if not candidate_doc_ids:\n",
    "        return {\"top_documents\": [], \"cosine_similarities\": [], \"top_documents_indices\": []}\n",
    "\n",
    "    doc_id_to_index = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
    "    candidate_indices = [doc_id_to_index[doc_id] for doc_id in candidate_doc_ids if doc_id in doc_id_to_index]\n",
    "\n",
    "    if not candidate_indices:\n",
    "        return {\"top_documents\": [], \"cosine_similarities\": [], \"top_documents_indices\": []}\n",
    "\n",
    "    scores = bm25_model.get_scores(expanded_query)\n",
    "    candidate_scores = [(i, scores[i]) for i in candidate_indices]\n",
    "    candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_candidates = candidate_scores[:initial_k]\n",
    "    top_indices = [i for i, _ in top_candidates]\n",
    "    top_doc_ids = [doc_ids[i] for i in top_indices]\n",
    "    doc_texts = load_documents_by_ids(dataset_path, top_doc_ids)\n",
    "\n",
    "    cross_encoder = get_cross_encoder()\n",
    "    cross_inputs = [(query, doc_texts[doc_ids[i]]) for i in top_indices]\n",
    "    rerank_scores = cross_encoder.predict(cross_inputs, batch_size=16)\n",
    "\n",
    "    reranked = sorted(zip(top_doc_ids, rerank_scores, top_indices), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    top_documents = []\n",
    "    cosine_similarities = []\n",
    "    top_documents_indices = []\n",
    "\n",
    "    for doc_id, score, idx in reranked:\n",
    "        top_documents.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"score\": float(score),\n",
    "            \"text\": doc_texts[doc_id]\n",
    "        })\n",
    "        cosine_similarities.append(float(score))\n",
    "        top_documents_indices.append(idx)\n",
    "\n",
    "    return {\n",
    "        \"top_documents\": top_documents,\n",
    "        \"cosine_similarities\": cosine_similarities,\n",
    "        \"top_documents_indices\": top_documents_indices\n",
    "    }\n",
    "\n",
    "#--- دوال التقييم\n",
    "\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_map_scores = []\n",
    "all_mrrs = []\n",
    "\n",
    "def calculate_precision_recall(relevantOrNot, retrievedDocument, threshold=0.5):\n",
    "    binaryResult = (retrievedDocument >= threshold).astype(int)\n",
    "    precision = precision_score(relevantOrNot, binaryResult, average='micro')\n",
    "    recall = recall_score(relevantOrNot, binaryResult, average='micro')\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_map_score(relevantOrNot, retrievedDocument):\n",
    "    return average_precision_score(relevantOrNot, retrievedDocument, average='micro')\n",
    "\n",
    "def calculate_mrr(y_true):\n",
    "    rank_position = np.where(y_true == 1)[0]\n",
    "    if len(rank_position) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (rank_position[0] + 1)\n",
    "\n",
    "def load_queries(queries_paths):\n",
    "    queries = []\n",
    "    for file_path in queries_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    query = json.loads(line.strip())\n",
    "                    if 'query' in query:\n",
    "                        queries.append(query)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "    return queries\n",
    "\n",
    "#--- مثال كامل للاستخدام\n",
    "\n",
    "dataset_path = \"lotte/lifestyle/dev/forum\"\n",
    "\n",
    "def search_function(query, top_n=10):\n",
    "    return search_in_bm25(query, dataset_path, top_k=top_n, initial_k=30)\n",
    "\n",
    "evaluate_search(dataset_path, search_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e000e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loaded_cache.clear()\n",
    "load_bert_model.cache_clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2850229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نتائج التقييم لـ lotte/lifestyle/dev/forum\n",
      "Execution Time: 9178.94 s\n",
      "Avg Precision  : 0.4691\n",
      "Avg Recall     : 0.4691\n",
      "Avg MAP        : 0.7245\n",
      "Avg MRR        : 0.7826\n"
     ]
    }
   ],
   "source": [
    "# خلية 1: استيراد المكتبات اللازمة\n",
    "# ----------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup   # موجودة أصلاً إن احتجتها لاحقاً\n",
    "import unicodedata\n",
    "import contractions\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# خلية 2: كلاس TextProcessor\n",
    "# ----------------------------------------------------------\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def cleaned_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalization_example(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def stemming_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        return ' '.join(self.stemmer.stem(w) for w in words)\n",
    "\n",
    "    def lemmatization_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        return ' '.join(self.lemmatizer.lemmatize(w) for w in words)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        return ' '.join(w for w in words if w.lower() not in self.stop_words)\n",
    "\n",
    "    def number_to_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        converted = []\n",
    "        for w in words:\n",
    "            if w.isdecimal() and w.isascii():\n",
    "                try:\n",
    "                    num = int(w)\n",
    "                    if num <= 999_999_999_999_999:\n",
    "                        converted.append(self.inflect_engine.number_to_words(w))\n",
    "                    else:\n",
    "                        converted.append('[Number Out of Range]')\n",
    "                except (ValueError, inflect.NumOutOfRangeError):\n",
    "                    converted.append('[Number Out of Range]')\n",
    "            else:\n",
    "                converted.append(w)\n",
    "        return ' '.join(converted)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        words, out, negate = self.tokenizer.tokenize(text), [], False\n",
    "        for w in words:\n",
    "            if w.lower() in ['not', \"n't\"]:\n",
    "                negate = True\n",
    "            elif negate:\n",
    "                out.append(f'NOT_{w}')\n",
    "                negate = False\n",
    "            else:\n",
    "                out.append(w)\n",
    "        return ' '.join(out)\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if text is None:\n",
    "            return text\n",
    "        text = self.cleaned_text(text)\n",
    "        text = self.normalization_example(text)\n",
    "        text = self.stemming_example(text)\n",
    "        text = self.lemmatization_example(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.number_to_words(text)\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.normalize_unicode(text)\n",
    "        text = self.handle_negations(text)\n",
    "        text = self.remove_urls(text)\n",
    "        return text\n",
    "\n",
    "processor = TextProcessor()\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# خلية 3: جلب البيانات من MongoDB\n",
    "# ----------------------------------------------------------\n",
    "def get_data_from_mongo(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection = db[dataset_path.replace(\"/\", \"_\")]\n",
    "\n",
    "    pids, texts = [], []\n",
    "    for doc in collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1}):\n",
    "        if \"doc_id\" in doc and \"text\" in doc and isinstance(doc[\"text\"], str):\n",
    "            pids.append(str(doc[\"doc_id\"]))\n",
    "            texts.append(doc[\"text\"])\n",
    "\n",
    "    df = pd.DataFrame({\"pid\": pids, \"text\": texts})\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    return df\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# خلية 4: تحميل النماذج وتعريف وظائف مساعدة\n",
    "# ----------------------------------------------------------\n",
    "retrieval_model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "cross_encoder   = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "processor       = TextProcessor()\n",
    "\n",
    "def load_documents_map(dataset_path):\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[\"information_retrieval\"]\n",
    "    collection = db[dataset_path.replace(\"/\", \"_\")]\n",
    "    doc_map = {}\n",
    "    for d in collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"text\": 1}):\n",
    "        if \"doc_id\" in d and \"text\" in d:\n",
    "            doc_map[str(d[\"doc_id\"])] = d[\"text\"]\n",
    "    return doc_map\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# خلية 5: search_in_bert (جديدة بدون FAISS)\n",
    "# ----------------------------------------------------------\n",
    "# كاش محلي للإيمبدنغز والـ doc_ids\n",
    "embedding_cache = {}\n",
    "\n",
    "def load_embeddings_and_doc_ids(dataset_path):\n",
    "    if dataset_path in embedding_cache:\n",
    "        return embedding_cache[dataset_path]\n",
    "\n",
    "    db_dir = os.path.join(r\"C:\\Users\\USER\\Desktop\\IR_Final_Project\\db\",\n",
    "                          dataset_path.replace(\"/\", \"__\"))\n",
    "    embeddings = joblib.load(os.path.join(db_dir, \"bert_embeddings.joblib\"))\n",
    "    doc_ids    = joblib.load(os.path.join(db_dir, \"bert_doc_ids.joblib\"))\n",
    "\n",
    "    embedding_cache[dataset_path] = (embeddings, doc_ids)\n",
    "    return embeddings, doc_ids\n",
    "\n",
    "\n",
    "def search_in_bert(query, dataset_path, top_k=50, rerank_k=10):\n",
    "    embeddings, doc_ids = load_embeddings_and_doc_ids(dataset_path)\n",
    "    doc_map             = load_documents_map(dataset_path)\n",
    "    doc_id_to_index     = {str(d): i for i, d in enumerate(doc_ids)}\n",
    "\n",
    "    query_vec = retrieval_model.encode(\n",
    "        processor.preprocess(query),\n",
    "        normalize_embeddings=True\n",
    "    ).reshape(1, -1)\n",
    "\n",
    "    similarities = cosine_similarity(query_vec, embeddings)[0]        # (N,)\n",
    "    top_idx      = np.argsort(similarities)[::-1][:top_k]\n",
    "    top_doc_ids  = [doc_ids[i] for i in top_idx]\n",
    "    top_docs     = [(str(d), doc_map.get(str(d), \"\")) for d in top_doc_ids]\n",
    "\n",
    "    filtered_docs = [(d, t) for d, t in top_docs if t.strip()]\n",
    "    pairs         = [(query, t) for _, t in filtered_docs]\n",
    "    rerank_scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    ranked   = sorted(zip(filtered_docs, rerank_scores),\n",
    "                      key=lambda x: x[1],\n",
    "                      reverse=True)[:rerank_k]\n",
    "\n",
    "    top_documents, cos_sims, top_doc_indices = [], [], []\n",
    "    for (d, t), s in ranked:\n",
    "        top_documents.append({\"doc_id\": d, \"score\": float(s), \"text\": t})\n",
    "        cos_sims.append(float(s))\n",
    "        top_doc_indices.append(doc_id_to_index.get(d, -1))\n",
    "\n",
    "    return {\n",
    "        \"top_documents\":         top_documents,\n",
    "        \"cosine_similarities\":   cos_sims,\n",
    "        \"top_documents_indices\": top_doc_indices\n",
    "    }\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# خلية 6: دوال التقييم\n",
    "# ----------------------------------------------------------\n",
    "all_precisions, all_recalls, all_map_scores, all_mrrs = [], [], [], []\n",
    "\n",
    "def calculate_precision_recall(y_true, y_score, thresh=0.5):\n",
    "    bin_res = (y_score >= thresh).astype(int)\n",
    "    return (precision_score(y_true, bin_res, average='micro'),\n",
    "            recall_score(y_true, bin_res, average='micro'))\n",
    "\n",
    "def calculate_map_score(y_true, y_score):\n",
    "    return average_precision_score(y_true, y_score, average='micro')\n",
    "\n",
    "def calculate_mrr(y_true):\n",
    "    pos = np.where(y_true == 1)[0]\n",
    "    return 0 if len(pos) == 0 else 1 / (pos[0] + 1)\n",
    "\n",
    "def load_queries(paths):\n",
    "    out = []\n",
    "    for p in paths:\n",
    "        with open(p, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    q = json.loads(line.strip())\n",
    "                    if 'query' in q: out.append(q)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {p}: {line}\")\n",
    "    return out\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# خلية 7: evaluate_search\n",
    "# ----------------------------------------------------------\n",
    "def evaluate_search(dataset_path, search_function):\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    df = get_data_from_mongo(dataset_path)\n",
    "\n",
    "    if dataset_path == 'lotte/lifestyle/dev/forum':\n",
    "        q_path = r'C:\\Users\\USER\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\qas.search.jsonl'\n",
    "    elif dataset_path == 'antique/train':\n",
    "        q_path = r'C:\\Users\\USER\\.ir_datasets\\antique\\test\\Answers.jsonl'\n",
    "    else:\n",
    "        print('⚠️ لم يُضبط مسار الاستعلامات لهذا الـ dataset.')\n",
    "        return\n",
    "\n",
    "    queries = load_queries([q_path])\n",
    "\n",
    "    for q in queries:\n",
    "        if 'query' not in q: continue\n",
    "\n",
    "        res  = search_function(q['query'], top_n=10)\n",
    "        sims = np.array(res['cosine_similarities'])\n",
    "        idxs = res['top_documents_indices']\n",
    "        rel  = np.zeros(len(df))\n",
    "        for pid in q.get('answer_pids', []):\n",
    "            rel[np.where(df['pid'] == str(pid))[0]] = 1\n",
    "\n",
    "        y_true = rel[idxs]\n",
    "        if y_true.sum() == 0: continue\n",
    "\n",
    "        p, r = calculate_precision_recall(y_true, sims)\n",
    "        all_precisions.append(p)\n",
    "        all_recalls.append(r)\n",
    "        all_map_scores.append(calculate_map_score(y_true, sims))\n",
    "        all_mrrs.append(calculate_mrr(y_true))\n",
    "\n",
    "    if not all_precisions:\n",
    "        print('⚠️ No valid queries evaluated.')\n",
    "        return\n",
    "\n",
    "    print(f'نتائج التقييم لـ {dataset_path}')\n",
    "    print(f'Execution Time: {time.time() - start:.2f} s')\n",
    "    print(f'Avg Precision  : {np.mean(all_precisions):.4f}')\n",
    "    print(f'Avg Recall     : {np.mean(all_recalls):.4f}')\n",
    "    print(f'Avg MAP        : {np.mean(all_map_scores):.4f}')\n",
    "    print(f'Avg MRR        : {np.mean(all_mrrs):.4f}')\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# خلية 8: مثال تشغيل\n",
    "# ----------------------------------------------------------\n",
    "dataset_path = 'lotte/lifestyle/dev/forum'\n",
    "\n",
    "def search_function(query, top_n=10):\n",
    "    return search_in_bert(query, dataset_path, top_k=50, rerank_k=top_n)\n",
    "\n",
    "# لتشغيل التقييم:\n",
    "evaluate_search(dataset_path, search_function)\n",
    "# ----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22fba05",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">Note!</span>\n",
    "<span style=\"color:yellow;\">cell evaluation ordering</span>\n",
    "\n",
    "<ol style=\"line-height:1.4\">\n",
    "  <li>tfidf</li>\n",
    "  <li>bert with vector store</li>\n",
    "  <li>hybrid</li>\n",
    "  <li>bm25</li>\n",
    "  <li>bert</li>\n",
    "</ol>\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
